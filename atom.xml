<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://zhangman123456.github.io</id>
    <title>张曼</title>
    <updated>2020-07-20T03:10:02.693Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://zhangman123456.github.io"/>
    <link rel="self" href="https://zhangman123456.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://zhangman123456.github.io/images/avatar.png</logo>
    <icon>https://zhangman123456.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 张曼</rights>
    <entry>
        <title type="html"><![CDATA[KNN算法]]></title>
        <id>https://zhangman123456.github.io/post/knn-suan-fa/</id>
        <link href="https://zhangman123456.github.io/post/knn-suan-fa/">
        </link>
        <updated>2020-07-16T08:45:34.000Z</updated>
        <content type="html"><![CDATA[<p>#算法简述<br>
邻近算法，或者说K最近邻（KNN，K-NearestNeighbor）分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻近值来代表。近邻算法就是将数据集合中每一个记录进行分类的方法。<br>
我们可以通过几幅图来形象地了解KNN算法<img src="https://zhangman123456.github.io/post-images/1594891375739.png" alt="" loading="lazy">毫无疑问，k值的选定对于分类的效果至关重要。如果K值过小，拟合的效果不好；如果K值过大，就会覆盖过多的样本数据，失去拟合的意义。<br>
要度量空间中点距离的话，有好几种度量方式，比如常见的曼哈顿距离计算，欧式距离（或欧几里得距离）等等。不过通常KNN算法中使用的是欧式距离，这里只是简单说一下，拿二维平面为例，，二维空间两个点的欧式距离计算公式如下：<img src="https://zhangman123456.github.io/post-images/1594891829868.jpg" alt="" loading="lazy">这样一来，不难得出，KNN算法最简单粗暴的方式就是计算预测点和所有样本点之间的距离，选出前K个距离最小的点看看哪个种类的点的数量最多。<br>
<strong>三.KNN特点</strong><br>
KNN是一种<strong>非参</strong>的，<strong>惰性</strong>的算法模型。什么是非参，什么是惰性呢？<br>
<strong>非参</strong>的意思并不是说这个算法不需要参数，而是意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。</p>
<p><strong>惰性</strong>又是什么意思呢？想想看，同样是分类算法，逻辑回归需要先对数据进行大量训练（tranning），最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。</p>
<p><strong>KNN算法的优势和劣势</strong><br>
了解KNN算法的优势和劣势，可以帮助我们在选择学习算法的时候做出更加明智的决定。那我们就来看看KNN算法都有哪些优势以及其缺陷所在！<br>
<strong>KNN算法优点</strong><br>
简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。<br>
模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。<br>
预测效果好。<br>
对异常值不敏感<br>
<strong>KNN算法缺点</strong><br>
对内存要求较高，因为该算法存储了所有训练数据<br>
预测阶段可能很慢<br>
对不相关的功能和数据规模敏感</p>
<p>KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。因此，基于python已经有很多成熟的可以直接使用的库来进行算法的实现<br>
KNN算法的简单实现：（调用sklearn自带的库）</p>
<pre><code>from sklearn import neighbors #sklearn中的函数集
from sklearn import datasets  #sklearn中的数据集

knn = neighbors.KNeighborsClassifier() # 申明对象
iris = datasets.load_iris()  # 导入数据（一个数据集，很大的数据集）
print(iris)

knn.fit(iris.data,iris.target) # 生成KNN模型，fit()函数可以理解为一个训练的过程

predicit_label = knn.predict([[0.2,0.3,0.3,0.2]]) # 预测这个数组的概率
print(predicit_label)
</code></pre>
<p>KNN算法的具体实现：<br>
（也是copy别人的代码，自己加的注释）</p>
<pre><code>'''
导入数据
filename数据存储路径
radio，按指定比例将数据划分为训练集和测试集
trainSet：训练数据
testSet：测试数据
'''
def loadDateset(filename,radio,trainSet=[],testSet=[]):
    with open(filename,'rt') as csvfile:   #打开filename；csvfile是一种文件格式
        lines = csv.reader(csvfile)   # 逐行读取数据
        dataset = list(lines)        # 将文件中的样本数据转换为列表类型存储
        for x in range(len(dataset)-1):  # 循环每行数据，将前4个特征值存入数组
            for y in range(4):
                dataset[x][y] = float(dataset[x][y])#也就是将前四列更新成浮点数
            if random.random()&lt;radio:    # 根据radio的大小将样本数据集划分为训练数据和测试数据
                trainSet.append(dataset[x])
            else:
                testSet.append(dataset[x])
'''
计算2个样例之间的距离（欧氏距离），length表示数据的维度
'''
def evaluateDistance(instance1,instance2,length):#变量为两个样例，还有维度
    distance = 0#初始化距离
    for x in range(length): # 循环每一维度，数值相减并对其平方，然后进行累加
        distance += pow((instance1[x]-instance2[x]),2)
    return math.sqrt(distance) # 开方求距离

'''
对于一个实例，找到离他最近的k个实例
'''
def getNeighbors(trainSet,testInstance,k):
    distance = []
    length = len(testInstance)-1 # 每个测试实例的维度（比如（2,3,4）就是三个维度）
    for x in range(len(trainSet)-1): 
        dist = evaluateDistance(testInstance,trainSet[x],length)# 分别计算一个测试实例到每一个训练数据的距离
        distance.append((trainSet[x],dist)) # 将每一个训练实例和其对应到测试实例的距离存储到列表（append（）函数用于在列表末尾增添新的对象）
    distance.sort(key=operator.itemgetter(1)) #进行排序，而排序的关键是元素第二维数据的大小，也就是dist
    neighbors = [] # 用来存储离一个实例最近的几个测试数据
    for x in range(k): # 取distance中前k个实例存储到neighbors
        neighbors.append(distance[x][0])#已经经过排序的distance列表，取出前k个元素，就是一个测试实例的k个近邻
        return neighbors

'''
在最近的K个实例中投票，少数服从多数，把要预测的实例归到多数那一类
'''
def getResponse(neighbors):#neighbors作为变量
    classvotes = {} # 定义一个字典，存储每一类别的数目
    for x in range(len(neighbors)):#遍历每一个k近邻的距离值大小
        response = neighbors[x][-1]#表示第x行最后一列的元素
        if response in classvotes:
            classvotes[response] += 1
        else:
            classvotes[response] = 1
    sortedVotes = sorted(classvotes.items(),key=operator.itemgetter(1),reverse=True) # 排序，输出数目最大的类别
    return sortedVotes[0][0]

'''
计算测试集的准确率
'''
def getAccuracy(testSet,predictions):
    correct = 0
    for x in range(len(testSet)):
        if testSet[x][-1] == predictions[x]: # 每行测试用例最后一列的标签与预测标签是否相等
            correct += 1
    return (correct/float(len(testSet)))*100.0

def main():
    trainSet = [] # 存储训练集
    testSet = []  # 存储测试集
    radio = 0.80  # 按4：1划分
    loadDateset('G:/PycharmProjects/Machine_Learning/KNN/irisdata.txt',radio,trainSet,testSet) #导入数据并划分
    print(&quot;trainSetNum: &quot;+ str(len(trainSet)))
    print(&quot;testSetNum: &quot;+  str(len(testSet)))
    predictions = []
    k = 3 # 选取前k个最近的实例
    for x in range(len(testSet)): # 循环预测测试集合的每个实例
        neighbors = getNeighbors(trainSet,testSet[x],k)
        result = getResponse(neighbors)
        predictions.append(result)
        print('&gt;predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))
    accuracy = getAccuracy(testSet,predictions)
    print('Accuracy: ' + repr(accuracy) + '%')

if __name__ == '__main__':
    main()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[线性回归作业]]></title>
        <id>https://zhangman123456.github.io/post/xian-xing-hui-gui-zuo-ye/</id>
        <link href="https://zhangman123456.github.io/post/xian-xing-hui-gui-zuo-ye/">
        </link>
        <updated>2020-07-13T09:32:17.000Z</updated>
        <content type="html"><![CDATA[<p>1.该<u>样本回归方程</u>就是我我们需要拟合的曲线：<br>
                     <strong>h(x) = θ<sub>0</sub> + θ<sub>1</sub>x</strong>；<br>
2.得出关于该回归方程的误差函数:<img src="https://zhangman123456.github.io/post-images/1594575102477.gif" alt="" loading="lazy"><br>
3.对误差函数求导：<img src="https://zhangman123456.github.io/post-images/1594632959520.gif" alt="" loading="lazy"><br>
<img src="https://zhangman123456.github.io/post-images/1594575689326.gif" alt="" loading="lazy"><br>
4.之后更新的到的θ<sub>0</sub> 和 θ<sub>1</sub><img src="https://zhangman123456.github.io/post-images/1594632981086.gif" alt="" loading="lazy"><br>
<img src="https://zhangman123456.github.io/post-images/1594575337724.gif" alt="" loading="lazy">（公式里的α是学习率，我们可以象征性地理解为步长）<br>
5.接下来，利用梯度下降的方法。这里我们可以分别使用批量梯度下降和随机梯度下降的方法来实现**（有助于更加具体地理解这两种梯度下降的算法）**<br>
（1）批量梯度下降算法<img src="https://zhangman123456.github.io/post-images/1594577818861.png" alt="" loading="lazy"><u>（上标i代表具体的某一个数据；x<sub>j</sub>代表系数θ<sub>j</sub>对应的因变量）</u><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，都需要把全部的样本都使用一遍，这样重复迭代直到θ<sub>j</sub>收敛为止。所以说该算法的复杂度是O(i（j+1）)<br>
（为了防止我忘了，我写了一个一看就懂的版本）<br>
  <img src="https://zhangman123456.github.io/post-images/1594579705255.jpg" alt="" loading="lazy"><br>
（2）随机梯度下降法<img src="https://zhangman123456.github.io/post-images/1594579776426.png" alt="" loading="lazy"><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，只使用所有样本中的一个数据，一旦到达最大的迭代次数或是满足预期的精度，就停止。这样算法的复杂度为O(j+1)，就下降了很多。<br>
  <u>（还是一个一看就会的版本）</u><img src="https://zhangman123456.github.io/post-images/1594580748171.jpg" alt="" loading="lazy"></p>
<pre><code>#批量梯度下降
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
 
#数据
a = np.random.standard_normal((1, 500))
x = [150,200,250,300,350,400,600]#自变量x
y = [6450,7450,8450,9450,11450,15450,18450]#因变量y
y = y - a*10
y = y[0]
def Optimization(x,y,theta,learning_rate):
    for i in range(iter):#批量梯度下降算法的核心，iter是最大迭代的次数。
        theta = Updata(x,y,theta,learning_rate)更新参数用的函数
    return theta#这是全部迭代之后的最后一组参数值 
def Updata(x,y,theta,learning_rate):
    m = len(x)#因变量的长度，即每次迭代的次数
    sum = 0.0
    sum1 = 0.0
    alpha = learning_rate
    h = 0
    for i in range(m):#每一次迭代，都用到全部的样本数据
        h = theta[0] + theta[1] * x[i]#一元线性回归方程，即我们需要拟合的曲线。
        sum += (h - y[i])
        sum1 += (h - y[i]) * x[i]
    theta[0] -= alpha * sum / m #更新theta[0]
    theta[1] -= alpha * sum1 / m #更新theta[1]
    return theta#每一次迭代都输出更新后的参数
 
#数据初始化
learning_rate = 0.001#学习率
theta = [0,0]#最开始的参数值
iter = 1000#循环的次数
theta = Optimization(x,y,theta,learning_rate)
 
plt.rcParams['font.sans-serif']=['SimHei']#该函数用来定义图形的各种默认属性，比如字符显示，线条样式，窗口大小，坐标轴宽度等等属性
plt.rcParams['axes.unicode_minus'] = False
'''
plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('样本',fontsize=40)
plt.savefig(&quot;样本.jpg&quot;)
'''
#可视化
b = np.arange(0,50)
c = theta[0] + b * theta[1]
 
plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.plot(b,c)
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('结果',fontsize=40)
plt.savefig(&quot;结果.jpg&quot;)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[线性回归]]></title>
        <id>https://zhangman123456.github.io/post/xian-xing-hui-gui/</id>
        <link href="https://zhangman123456.github.io/post/xian-xing-hui-gui/">
        </link>
        <updated>2020-07-11T10:59:13.000Z</updated>
        <content type="html"><![CDATA[<p>1.该<u>样本回归方程</u>就是我我们需要拟合的曲线：<br>
                     <strong>h(x) = θ<sub>0</sub> + θ<sub>1</sub>x</strong>；<br>
2.得出关于该回归方程的误差函数:<img src="https://zhangman123456.github.io/post-images/1594575102477.gif" alt="" loading="lazy"><br>
3.对误差函数求导：<br>
<img src="https://zhangman123456.github.io/post-images/1594575689326.gif" alt="" loading="lazy"><br>
4.之后更新的到的θ<sub>0</sub> 和 θ<sub>1</sub><img src="https://zhangman123456.github.io/post-images/1594575337724.gif" alt="" loading="lazy">（公式里的α是学习率，我们可以象征性地理解为步长）<br>
5.接下来，利用梯度下降的方法。这里我们可以分别使用批量梯度下降和随机梯度下降的方法来实现**（有助于更加具体地理解这两种梯度下降的算法）**<br>
（1）批量梯度下降算法<img src="https://zhangman123456.github.io/post-images/1594577818861.png" alt="" loading="lazy"><u>（上标i代表具体的某一个数据；x<sub>j</sub>代表系数θ<sub>j</sub>对应的因变量）</u><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，都需要把全部的样本都使用一遍，这样重复迭代直到θ<sub>j</sub>收敛为止。所以说该算法的复杂度是O(i（j+1）)<br>
（为了防止我忘了，我写了一个一看就懂的版本）<br>
  <img src="https://zhangman123456.github.io/post-images/1594579705255.jpg" alt="" loading="lazy"><br>
（2）随机梯度下降法<img src="https://zhangman123456.github.io/post-images/1594579776426.png" alt="" loading="lazy"><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，只使用所有样本中的一个数据，一旦到达最大的迭代次数或是满足预期的精度，就停止。这样算法的复杂度为O(j+1)，就下降了很多。<br>
  <u>（还是一个一看就会的版本）</u><img src="https://zhangman123456.github.io/post-images/1594580748171.jpg" alt="" loading="lazy"></p>
<pre><code>批量梯度下降算法
@author: 
&quot;&quot;&quot;

import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
 
#数据
a = np.random.standard_normal((1, 500))
x = [150,200,250,300,350,400,600]#自变量x
y = [6450,7450,8450,9450,11450,15450,18450]#因变量y
y = y - a*10
y = y[0]
 
def Optimization(x,y,theta,learning_rate):
    for i in range(iter):#批量梯度下降算法的核心，iter是最大迭代的次数。
        theta = Updata(x,y,theta,learning_rate)更新参数用的函数
    return theta#这是全部迭代之后的最后一组参数值 
def Updata(x,y,theta,learning_rate):
    m = len(x)#因变量的长度，即每次迭代的次数
    sum = 0.0
    sum1 = 0.0
    alpha = learning_rate
    h = 0
    for i in range(m):#每一次迭代，都用到全部的样本数据
        h = theta[0] + theta[1] * x[i]#一元线性回归方程，即我们需要拟合的曲线。
        sum += (h - y[i])
        sum1 += (h - y[i]) * x[i]
    theta[0] -= alpha * sum / m #更新theta[0]
    theta[1] -= alpha * sum1 / m #更新theta[1]
    return theta#每一次迭代都输出更新后的参数
#数据初始化
learning_rate = 0.001#学习率
theta = [0,0]#最开始的参数值
iter = 1000#循环的次数
theta = Optimization(x,y,theta,learning_rate)
 
plt.rcParams['font.sans-serif']=['SimHei']#该函数用来定义图形的各种默认属性，比如字符显示，线条样式，窗口大小，坐标轴宽度等等属性
plt.rcParams['axes.unicode_minus'] = False

plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('样本',fontsize=40)
plt.savefig(&quot;样本.jpg&quot;)
'''
#可视化
b = np.arange(0,50)
c = theta[0] + b * theta[1]
 
plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.plot(b,c)
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('结果',fontsize=40)
plt.savefig(&quot;结果.jpg&quot;)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[梯度下降法]]></title>
        <id>https://zhangman123456.github.io/post/ti-du-xia-jiang-fa-yi/</id>
        <link href="https://zhangman123456.github.io/post/ti-du-xia-jiang-fa-yi/">
        </link>
        <updated>2020-07-11T01:15:53.000Z</updated>
        <content type="html"><![CDATA[<p>#基本思想<br>
假设我们爬山，如果想最快的上到山顶，那么我们应该从山势最陡的地方上山。也就是山势变化最快的地方上山<br>
同样，如果从任意一点出发，需要最快搜索到函数最大值，那么我们也应该从函数变化最快的方向搜索。<br>
函数变化最快的方向是什么呢？函数的<strong>梯度</strong>。<br>
假如函数为一元函数，<strong>梯度</strong>就是该函数的倒数<img src="https://zhangman123456.github.io/post-images/1594430603494.png" alt="" loading="lazy"><br>
如果为二元函数，梯度定义为：<img src="https://zhangman123456.github.io/post-images/1594430651348.png" alt="" loading="lazy">（此处用到高数偏导数的知识来求二元函数的倒数）<br>
如果需要找的是函数极小点，那么应该从负梯度的方向寻找，该方法称之为梯度下降法。<br>
<img src="https://zhangman123456.github.io/post-images/1594430871728.png" alt="" loading="lazy">要搜索极小值C点，在A点必须向x增加方向搜索，此时与A点梯度方向相反；在B点必须向x减小方向搜索，此时与B点梯度方向相反。总之，搜索极小值，必须向负梯度方向搜索。<br>
假设函数 y = f(x1,x2,......,xn) 只有一个极小点。初始给定参数为  X0 = (x10,x20,......xn0) 从这个点如何搜索才能找到原函数的极小值点？<br>
方法：</p>
<ol>
<li>首先设定一个较小的正数η，ε;</li>
<li>求当前位置处的各个偏导数：<img src="https://zhangman123456.github.io/post-images/1594431281163.png" alt="" loading="lazy"></li>
<li>修改当前函数的参数值，公式如下：<br>
<img src="https://zhangman123456.github.io/post-images/1594431327433.png" alt="" loading="lazy"></li>
<li>如果参数变化量小于，退出；否则返回2。<br>
例：任给一个初始出发点，设为x0=-4，利用梯度下降法求函数y=x²/2-2x的极小值。<br>
准备工作：<br>
（1）给定两个参数值η = 0.9，ε = 0.01（η即为学习效率，或者步长）<br>
（2）计算导数：dy/dx = x - 2;<br>
一.（1）计算当前导数值：y' = -4 - 2 = -6；<br>
   （2）修改当前的参数值：x = x-ηy' = -4 - 0.9*(-6) = 1.4;<br>
  （3）计算△x = -0.9 * （-6）= 5.4。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环<br>
二.（1）计算当前导数值：y' = 1.4 - 2 = -0.6；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.4 - 0.9*(-0.6)= 1.94;<br>
  （3）计算△x = -0.9 * （-0.6）= 0.54。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环。<br>
三.（1）计算当前导数值：y' = 1.94 - 2 = -0.06；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.94 - 0.9*(-0.06)= 1.994;<br>
  （3）计算△x = -0.9 * （-0.06）= 0.054。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环。<br>
四.（1）计算当前导数值：y' = 1.994 - 2 = -0.006；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.994 - 0.9*(-0.006)= 1.9994;<br>
  （3）计算△x = -0.9 * （-0.006）= 0.0054。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&lt;=ε，则终止循环，输出此时的x即为函数的极小点。<br>
python代码实现</li>
</ol>
<pre><code class="language-python">＃解决实例的Python代码
import numpy as np
inport matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams['font.family'] = 'sans-serif'
mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = false

#构建一维元素图形
def f1(x):
　　return 0.5 * (x ** 2) - 2x
def h1(x):
　　return x-2

#使用梯度下降法
GD_X = []
GD_Y = []
x = -4
alpha = 0.9
f_change = np.abs(alpha * hl(x))
f_current = f_change
GD_X.append(x)
GD_Y.append(f_current)

#迭代次数
iter_num = 0
while f_change &gt;0.01 and iter_num&lt;10:
　　iter_num+=1
　　x = x - x - alpha * h1(x)
　　tem = f1(x)
　　f_change = np.abs(alpha * hl(x))
　　f_current = tmp
　　GD_X.append(f_current)

print(u'最终的结果：(%.5f,%.5f)'%(x,f_current))
pirnt(u'迭代次数：%d'%iter_num)
printf(GD_X)
</code></pre>
<p>#损失函数<br>
学习了梯度下降之后，我们引入新的概念：<strong>损失函数</strong>。便于我们直接利用梯度下降法计算损失函数的最小值。<br>
我们可以近似地将<strong>损失函数</strong>理解为方差或者平方差。根据给出的一系类数据我们可以得出相关的函数，再根据该函数和不同变量，我们可以得出不同变量对应的不同的函数值。而每个函数值和每个真实的数值之间有一定的误差，根据这些误差我们可以得到相关的函数，即<strong>损失函数</strong>。<strong>损失函数</strong>的值越小，我们之前得到的函数就越精确。<br>
例：根据某组数据得到预测的函数：h(x) = θ₁ + θ₂x。只含有一个特征/输入变量。在这个式子中θ₁，θ₂都是未知参数，x是已知数据。我们的目的是得到合适的参数使得我们的预测尽可能准确。这时就需要引入损失函数的概念。<br>
<img src="https://zhangman123456.github.io/post-images/1594383086368.jpg" alt="" loading="lazy"><em>(暂时不用管1/2)</em><br>
上图中的J(θ₁,θ₂)是一个关于θ₁和θ₂的二元函数，也就是<strong>损失函数</strong>。在三维空间坐标系内，存在一对点（θ₁,θ₂）使得J(θ₁,θ₂)有最小值<br>
然后对损失函数进行一个简单的推导（利用梯度下降的算法）<br>
<img src="https://zhangman123456.github.io/post-images/1594444209052.jpg" alt="" loading="lazy">（在这个公式里突然多了一个α，它表示的是学习率，用来限定步长的大小，也很容易理解，毕竟得到的偏导是类似斜率的东西，只是一个方向，总得加个数值，才能表示向这个方向移动的距离。<br>
对于的取值，一般都取的比较小，但也不要太小，太小就意味着迭代步数要增加，运算时间边长。另外，迭代的步数要自己设置。<br>
原文链接：https://blog.csdn.net/i96jie/java/article/details/81206473）<br>
#梯度下降法的三个变种<br>
了解了梯度下降法的核心思想以及损失函数后，我们接着了解梯度下降法的三个变种<br>
1.批量梯度下降法（BGD）<br>
其需要计算整个训练集的梯度，即：<img src="https://i.loli.net/2019/08/12/zAiXFLUSyrd5qap.png" alt="" loading="lazy">其中η为学习率，用来控制更新的“力度”/&quot;步长&quot;。<br>
优点：<br>
    对于凸目标函数，可以保证全局最优； 对于非凸目标函数，可以保证一个局部最优。<br>
2.随机梯度下降算法（SGD）<br>
仅计算某个样本的梯度，即针对某一个训练样本 xi及其label yi更新参数：<img src="https://i.loli.net/2019/08/12/ItgcSRaT4jreKMl.png" alt="" loading="lazy">逐步减小学习率，SGD表现得同BGD很相似，最后都可以有不错的收敛。<br>
优点：<br>
    更新频次快，优化速度更快; 可以在线优化(可以无法处理动态产生的新样本)；一定的随机性导致有几率跳出局部最优(随机性来自于用一个样本的梯度去代替整体样本的梯度)。<br>
3.小批量梯度下降算法(MBGD),计算包含n个样本的mini-batch的梯度：<img src="https://i.loli.net/2019/08/12/ItgcSRaT4jreKMl.png" alt="" loading="lazy">MBGD是训练神经网络最常用的优化方法。<br>
优点：<br>
参数更新时的动荡变小，收敛过程更稳定，降低收敛难度；可以利用现有的线性代数库高效的计算多个样本的梯度。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://zhangman123456.github.io/post/hello-gridea/</id>
        <link href="https://zhangman123456.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>