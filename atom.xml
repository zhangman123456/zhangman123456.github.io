<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://zhangman123456.github.io</id>
    <title>张曼</title>
    <updated>2020-07-13T01:45:52.571Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://zhangman123456.github.io"/>
    <link rel="self" href="https://zhangman123456.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://zhangman123456.github.io/images/avatar.png</logo>
    <icon>https://zhangman123456.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 张曼</rights>
    <entry>
        <title type="html"><![CDATA[线性回归]]></title>
        <id>https://zhangman123456.github.io/post/xian-xing-hui-gui/</id>
        <link href="https://zhangman123456.github.io/post/xian-xing-hui-gui/">
        </link>
        <updated>2020-07-11T10:59:13.000Z</updated>
        <content type="html"><![CDATA[<p>  在统计学中，<strong>线性回归</strong>是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。<br>
  只有一个自变量的情况称为<strong>简单回归</strong>,大于一个自变量情况的叫做<strong>多元回归</strong>。<br>
#<strong>简单回归（一元线性回归）</strong><br>
  一元线性回归模型如下：<img src="https://zhangman123456.github.io/post-images/1594468030322.jpg" alt="" loading="lazy"><em>θ₁和θ₂是常数，随机误差ε是无法直接观测的随机变量</em>。<br>
  如果我们对等式两边分别求期望（可以理解为求平均值）:<br>
                         <strong>E(y) = θ₁ + θ₂x</strong>；<br>
  我们通常将这个期望方程称为<strong>一元线性回归方程</strong>（或总体回归直线）；以<strong>E(y)</strong>\表示给定自变量值时因变量的均值或期望值。<br>
  一元线性回归方程中的<em>θ₁</em>和<em>θ₂</em>通常是不能通过观测得到的。找到两个样本统计量<em>a、b</em>分别作为参数<em>α，β</em>的估计量，那么用<em>a、b</em>分别替代总体回归方程中的参数<em>α，β</em>，则得到估计的回归方程，也称样本回归方程。一元线性的<u>样本回归方程</u>也称为<strong>样本回归直线</strong>，其形式如下：<br>
                            <strong>y = a + bx;</strong><br>
  式中，y是与x自变量取值相对应的因变量均值E(y)的估计；a和b分别为总体回归方程参数α和β的估计量，即回归直线在Y轴上的截距和回归直线的斜率。<br>
  得到了一元线性回归方程，我们可以开始用<strong>梯度下降法</strong>来逐步求解这个方程的两个系数。（这里不对梯度下降法做过多解释，详情可以参考我的上一篇博客）<br>
1.该<u>样本回归方程</u>就是我我们需要拟合的曲线：<br>
                     <strong>h(x) = θ<sub>0</sub> + θ<sub>1</sub>x</strong>；<br>
2.得出关于该回归方程的误差函数:<img src="https://zhangman123456.github.io/post-images/1594575102477.gif" alt="" loading="lazy"><br>
3.对误差函数求导：<img src="https://zhangman123456.github.io/post-images/1594575689326.gif" alt="" loading="lazy"><br>
4.之后更新的到的θ<sub>0</sub> 和 θ<sub>1</sub><img src="https://zhangman123456.github.io/post-images/1594575337724.gif" alt="" loading="lazy">（公式里的α是学习率，我们可以象征性地理解为步长）<br>
5.接下来，利用梯度下降的方法。这里我们可以分别使用批量梯度下降和随机梯度下降的方法来实现**（有助于更加具体地理解这两种梯度下降的算法）**<br>
（1）批量梯度下降算法<img src="https://zhangman123456.github.io/post-images/1594577818861.png" alt="" loading="lazy"><u>（上标i代表具体的某一个数据；x<sub>j</sub>代表系数θ<sub>j</sub>对应的因变量）</u><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，都需要把全部的样本都使用一遍，这样重复迭代直到θ<sub>j</sub>收敛为止。所以说该算法的复杂度是O(i（j+1）)<br>
（为了防止我忘了，我写了一个一看就懂的版本）<br>
  <img src="https://zhangman123456.github.io/post-images/1594579705255.jpg" alt="" loading="lazy"><br>
（2）随机梯度下降法<img src="https://zhangman123456.github.io/post-images/1594579776426.png" alt="" loading="lazy"><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，只使用所有样本中的一个数据，一旦到达最大的迭代次数或是满足预期的精度，就停止。这样算法的复杂度为O(j+1)，就下降了很多。<br>
  <u>（还是一个一看就会的版本）</u><img src="https://zhangman123456.github.io/post-images/1594580748171.jpg" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[梯度下降法]]></title>
        <id>https://zhangman123456.github.io/post/ti-du-xia-jiang-fa-yi/</id>
        <link href="https://zhangman123456.github.io/post/ti-du-xia-jiang-fa-yi/">
        </link>
        <updated>2020-07-11T01:15:53.000Z</updated>
        <content type="html"><![CDATA[<p>#基本思想<br>
假设我们爬山，如果想最快的上到山顶，那么我们应该从山势最陡的地方上山。也就是山势变化最快的地方上山<br>
同样，如果从任意一点出发，需要最快搜索到函数最大值，那么我们也应该从函数变化最快的方向搜索。<br>
函数变化最快的方向是什么呢？函数的<strong>梯度</strong>。<br>
假如函数为一元函数，<strong>梯度</strong>就是该函数的倒数<img src="https://zhangman123456.github.io/post-images/1594430603494.png" alt="" loading="lazy"><br>
如果为二元函数，梯度定义为：<img src="https://zhangman123456.github.io/post-images/1594430651348.png" alt="" loading="lazy">（此处用到高数偏导数的知识来求二元函数的倒数）<br>
如果需要找的是函数极小点，那么应该从负梯度的方向寻找，该方法称之为梯度下降法。<br>
<img src="https://zhangman123456.github.io/post-images/1594430871728.png" alt="" loading="lazy">要搜索极小值C点，在A点必须向x增加方向搜索，此时与A点梯度方向相反；在B点必须向x减小方向搜索，此时与B点梯度方向相反。总之，搜索极小值，必须向负梯度方向搜索。<br>
假设函数 y = f(x1,x2,......,xn) 只有一个极小点。初始给定参数为  X0 = (x10,x20,......xn0) 从这个点如何搜索才能找到原函数的极小值点？<br>
方法：</p>
<ol>
<li>首先设定一个较小的正数η，ε;</li>
<li>求当前位置处的各个偏导数：<img src="https://zhangman123456.github.io/post-images/1594431281163.png" alt="" loading="lazy"></li>
<li>修改当前函数的参数值，公式如下：<br>
<img src="https://zhangman123456.github.io/post-images/1594431327433.png" alt="" loading="lazy"></li>
<li>如果参数变化量小于，退出；否则返回2。<br>
例：任给一个初始出发点，设为x0=-4，利用梯度下降法求函数y=x²/2-2x的极小值。<br>
准备工作：<br>
（1）给定两个参数值η = 0.9，ε = 0.01（η即为学习效率，或者步长）<br>
（2）计算导数：dy/dx = x - 2;<br>
一.（1）计算当前导数值：y' = -4 - 2 = -6；<br>
   （2）修改当前的参数值：x = x-ηy' = -4 - 0.9*(-6) = 1.4;<br>
  （3）计算△x = -0.9 * （-6）= 5.4。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环<br>
二.（1）计算当前导数值：y' = 1.4 - 2 = -0.6；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.4 - 0.9*(-0.6)= 1.94;<br>
  （3）计算△x = -0.9 * （-0.6）= 0.54。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环。<br>
三.（1）计算当前导数值：y' = 1.94 - 2 = -0.06；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.94 - 0.9*(-0.06)= 1.994;<br>
  （3）计算△x = -0.9 * （-0.06）= 0.054。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环。<br>
四.（1）计算当前导数值：y' = 1.994 - 2 = -0.006；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.994 - 0.9*(-0.006)= 1.9994;<br>
  （3）计算△x = -0.9 * （-0.006）= 0.0054。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&lt;=ε，则终止循环，输出此时的x即为函数的极小点。<br>
python代码实现</li>
</ol>
<pre><code class="language-python">＃解决实例的Python代码
import numpy as np
inport matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams['font.family'] = 'sans-serif'
mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = false

#构建一维元素图形
def f1(x):
　　return 0.5 * (x ** 2) - 2x
def h1(x):
　　return x-2

#使用梯度下降法
GD_X = []
GD_Y = []
x = -4
alpha = 0.9
f_change = np.abs(alpha * hl(x))
f_current = f_change
GD_X.append(x)
GD_Y.append(f_current)

#迭代次数
iter_num = 0
while f_change &gt;0.01 and iter_num&lt;10:
　　iter_num+=1
　　x = x - x - alpha * h1(x)
　　tem = f1(x)
　　f_change = np.abs(alpha * hl(x))
　　f_current = tmp
　　GD_X.append(f_current)

print(u'最终的结果：(%.5f,%.5f)'%(x,f_current))
pirnt(u'迭代次数：%d'%iter_num)
printf(GD_X)
</code></pre>
<p>#损失函数<br>
学习了梯度下降之后，我们引入新的概念：<strong>损失函数</strong>。便于我们直接利用梯度下降法计算损失函数的最小值。<br>
我们可以近似地将<strong>损失函数</strong>理解为方差或者平方差。根据给出的一系类数据我们可以得出相关的函数，再根据该函数和不同变量，我们可以得出不同变量对应的不同的函数值。而每个函数值和每个真实的数值之间有一定的误差，根据这些误差我们可以得到相关的函数，即<strong>损失函数</strong>。<strong>损失函数</strong>的值越小，我们之前得到的函数就越精确。<br>
例：根据某组数据得到预测的函数：h(x) = θ₁ + θ₂x。只含有一个特征/输入变量。在这个式子中θ₁，θ₂都是未知参数，x是已知数据。我们的目的是得到合适的参数使得我们的预测尽可能准确。这时就需要引入损失函数的概念。<br>
<img src="https://zhangman123456.github.io/post-images/1594383086368.jpg" alt="" loading="lazy"><em>(暂时不用管1/2)</em><br>
上图中的J(θ₁,θ₂)是一个关于θ₁和θ₂的二元函数，也就是<strong>损失函数</strong>。在三维空间坐标系内，存在一对点（θ₁,θ₂）使得J(θ₁,θ₂)有最小值<br>
然后对损失函数进行一个简单的推导（利用梯度下降的算法）<br>
<img src="https://zhangman123456.github.io/post-images/1594444209052.jpg" alt="" loading="lazy">（在这个公式里突然多了一个α，它表示的是学习率，用来限定步长的大小，也很容易理解，毕竟得到的偏导是类似斜率的东西，只是一个方向，总得加个数值，才能表示向这个方向移动的距离。<br>
对于的取值，一般都取的比较小，但也不要太小，太小就意味着迭代步数要增加，运算时间边长。另外，迭代的步数要自己设置。<br>
原文链接：https://blog.csdn.net/i96jie/java/article/details/81206473）<br>
#梯度下降法的三个变种<br>
了解了梯度下降法的核心思想以及损失函数后，我们接着了解梯度下降法的三个变种<br>
1.批量梯度下降法（BGD）<br>
其需要计算整个训练集的梯度，即：<img src="https://i.loli.net/2019/08/12/zAiXFLUSyrd5qap.png" alt="" loading="lazy">其中η为学习率，用来控制更新的“力度”/&quot;步长&quot;。<br>
优点：<br>
    对于凸目标函数，可以保证全局最优； 对于非凸目标函数，可以保证一个局部最优。<br>
2.随机梯度下降算法（SGD）<br>
仅计算某个样本的梯度，即针对某一个训练样本 xi及其label yi更新参数：<img src="https://i.loli.net/2019/08/12/ItgcSRaT4jreKMl.png" alt="" loading="lazy">逐步减小学习率，SGD表现得同BGD很相似，最后都可以有不错的收敛。<br>
优点：<br>
    更新频次快，优化速度更快; 可以在线优化(可以无法处理动态产生的新样本)；一定的随机性导致有几率跳出局部最优(随机性来自于用一个样本的梯度去代替整体样本的梯度)。<br>
3.小批量梯度下降算法(MBGD),计算包含n个样本的mini-batch的梯度：<img src="https://i.loli.net/2019/08/12/ItgcSRaT4jreKMl.png" alt="" loading="lazy">MBGD是训练神经网络最常用的优化方法。<br>
优点：<br>
参数更新时的动荡变小，收敛过程更稳定，降低收敛难度；可以利用现有的线性代数库高效的计算多个样本的梯度。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://zhangman123456.github.io/post/hello-gridea/</id>
        <link href="https://zhangman123456.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>