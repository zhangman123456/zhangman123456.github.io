<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://zhangman123456.github.io</id>
    <title>张曼</title>
    <updated>2020-07-30T07:26:27.964Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://zhangman123456.github.io"/>
    <link rel="self" href="https://zhangman123456.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://zhangman123456.github.io/images/avatar.png</logo>
    <icon>https://zhangman123456.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 张曼</rights>
    <entry>
        <title type="html"><![CDATA[矩阵分解]]></title>
        <id>https://zhangman123456.github.io/post/ju-zhen-fen-jie/</id>
        <link href="https://zhangman123456.github.io/post/ju-zhen-fen-jie/">
        </link>
        <updated>2020-07-28T20:08:42.000Z</updated>
        <content type="html"><![CDATA[<p><strong>矩阵分解的概念</strong><br>
　　矩阵分解是指将一个矩阵分解成两个或者多个矩阵的乘积。譬如，一个N行M列的矩阵可以被分解为一个N行K列的矩阵和一个K行M列的矩阵的乘积（注意乘积顺序不能颠倒）。K可以理解为一个参数，这个参数会对这两个矩阵乘积产生的矩阵大小产生影响，但是不会影响乘积得到的矩阵的行数和列数。可以利用矩阵分解来解决很多实际生活中的问题。<br>
<strong>例：</strong><br>
看图中的这个例子：<img src="https://zhangman123456.github.io/post-images/1596021541880.png" alt="" loading="lazy">　　矩阵中，描述了5个用户(U1,U2,U3,U4 ,U5)对4个物品(D1,D2,D3,D4)的评分(1-5分)，- 表示没有评分，我们的目的是把没有评分的给预测出来，然后按预测的分数高低，给用户进行物品推荐。<br>
　　将图中的数据看作是一个维度为N×Ｍ的评分矩阵R，则R可以看做两个矩阵P(N×Ｋ)和Ｑ(Ｋ×Ｍ)的乘积。对于P,Q矩阵的解释，直观上，P矩阵是N个用户对K个主题的关系，Q矩阵是K个主题跟M个物品的关系，或者说，N个用户通过K个主题对M个物品进行评分，那么我们在模拟时，也要通过这K个主题进行模拟。至于K个主题具体是什么，在算法里面K是一个参数，需要调节的，通常10~100之间。<br>
具体步骤：<br>
　　矩阵的分解（R是真实的评分矩阵，R^是我们预测得到的矩阵。R与R^越接近，就证明我们取得两个矩阵P和Q的预测模型越好。N和M是确定的参数值不会改变，因此K的取值对整个模型的预测结果起着至关重要的作用)<img src="https://zhangman123456.github.io/post-images/1596076385604.png" alt="" loading="lazy"><br>
　　另外在这里补充说明一下矩阵相乘的规则：Ｒ＾矩阵的第ｉ行第ｊ列的元素 = P 矩阵第ｉ行的元素×Ｑ矩阵第ｊ列的元素。注意P和Q不能颠倒。<img src="https://zhangman123456.github.io/post-images/1596076633042.png" alt="" loading="lazy"><br>
　　损失函数的平方等于真实值与预测值之间差值的平方，可以直接把预测值替换成P和Q的乘积。所以损失函数的值越小，真实值与预测值之间的差距越小，也就意味着我们拟合的结果越接近真实值，也就是我们对矩阵分解的结果越好。所以接下来就是解决问题的核心：基于梯度下降的优化算法。利用梯度下降的方法，一步步逼近损失函数的最小值<img src="https://zhangman123456.github.io/post-images/1596076674246.jpg" alt="" loading="lazy"><br>
　　但是在机器学习中，我们通常在损失函数的后面加上一个正则项，公式如下：<img src="https://zhangman123456.github.io/post-images/1596076704081.png" alt="" loading="lazy"><br>
　　这样一来，我们也需要对算法做出一点改动<img src="https://zhangman123456.github.io/post-images/1596076729403.jpg" alt="" loading="lazy"><br>
<strong>代码实现</strong></p>
<pre><code>import numpy as np  
import matplotlib.pyplot as plt
 
 
def matrix(R, P, Q, K, alpha, beta):
    result=[]
    steps = 1
    while 1 :
    #使用梯度下降的一步步的更新P,Q矩阵直至得到最终收敛值
        steps = steps + 1    
        eR = np.dot(P,Q)
        e=0
        for i in range(len(R)):
            for j in range(len(R[i])):
                if R[i][j]&gt;0:
                    # .dot(P,Q) 表示矩阵内积,即Pik和Qkj k由1到k的和eij为真实值和预测值的之间的误差,
                    eij=R[i][j]-np.dot(P[i,:],Q[:,j]) 
                    #求误差函数值，我们在下面更新p和q矩阵的时候我们使用的是化简得到的最简式，较为简便，
                    #但下面我们仍久求误差函数值这里e求的是每次迭代的误差函数值，用于绘制误差函数变化图
                    e=e+pow(R[i][j] - np.dot(P[i,:],Q[:,j]),2) 
                    for k in range(K):
                        #在上面的误差函数中加入正则化项防止过拟合
                        e=e+(beta/2)*(pow(P[i][k],2)+pow(Q[k][j],2))
                        
                    for k in range(K):
                        #在更新p,q时我们使用化简得到了最简公式
                        P[i][k]=P[i][k]+alpha*(2*eij*Q[k][j]-beta*P[i][k])
                        Q[k][j]=Q[k][j]+alpha*(2*eij*P[i][k]-beta*Q[k][j])
        #print('迭代轮次:', steps, '   e:', e)
        result.append(e)#将每一轮更新的损失函数值添加到数组result末尾
        
        #当损失函数小于一定值时，迭代结束
        if eij&lt;0.00001:
            break
    return P,Q,result
   
R=[
   [5,3,0,1],
   [4,0,0,1],
   [1,1,0,5],
   [1,0,0,4],
   [0,1,5,4],
   ]
 
R=np.array(R)
    
alpha = 0.0001 #学习率
beta = 0.002 
 
N = len(R) #表示行数
M = len(R[0]) #表示列数
K = 3 #3个因子
 
p = np.random.rand(N, K) #随机生成一个 N行 K列的矩阵
q = np.random.rand(K, M) #随机生成一个 M行 K列的矩阵
 
P, Q, result=matrix(R, p, q, K,  alpha, beta)
print(&quot;矩阵R为：\n&quot;,R)
print(&quot;矩阵P为：\n&quot;,P)
print(&quot;矩阵Q为：\n&quot;,Q)
MF = np.dot(P,Q)
print(&quot;预测矩阵：\n&quot;,MF)
print()
#下面代码可以绘制损失函数的收敛曲线图
print(&quot;损失函数的收敛过程为：&quot;)
n=len(result)
x=range(n)
plt.plot(x, result,color='b',linewidth=3)
plt.xlabel(&quot;generation&quot;)
plt.ylabel(&quot;loss&quot;)
plt.show()
</code></pre>
<p>下面是运行的结果：</p>
<pre><code>矩阵R为：
 [[5 3 0 1]
 [4 0 0 1]
 [1 1 0 5]
 [1 0 0 4]
 [0 1 5 4]]
矩阵P为：
 [[0.11646557 2.20824426 0.88565195]
 [0.03596535 1.51168656 1.0372382 ]
 [1.6897674  0.11533018 1.13699815]
 [1.37709179 0.16820378 0.84690913]
 [1.50038103 1.43928107 0.61271558]]
矩阵Q为：
 [[-0.09628733 -0.1151309   1.07555694  2.32234973]
 [ 1.97215153  0.90817549  1.96709116 -0.04825554]
 [ 0.89544739  0.69563508  0.70044543  0.95484359]]
预测矩阵：
 [[5.13683287 2.60815509 5.08943398 1.00957287]
 [3.90660419 2.09027523 3.73883681 1.00097713]
 [1.08286742 0.7011314  2.8407112  5.00432095]
 [0.95748942 0.58335244 2.40522644 3.99863775]
 [3.24266723 1.56060602 4.87411612 4.00000375]]

损失函数的收敛过程为：
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://zhangman123456.github.io/post-images/1596076909331.PNG" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[推荐系统之协同过滤算法]]></title>
        <id>https://zhangman123456.github.io/post/tui-jian-xi-tong-zhi-xie-tong-guo-lu-suan-fa/</id>
        <link href="https://zhangman123456.github.io/post/tui-jian-xi-tong-zhi-xie-tong-guo-lu-suan-fa/">
        </link>
        <updated>2020-07-26T03:49:43.000Z</updated>
        <content type="html"><![CDATA[<h1 id="推荐算法">推荐算法</h1>
<h2 id="1什么是推荐算法">1.什么是推荐算法</h2>
<p>首先，在了解协同过滤算法之前，我们可以先了解一下<u>推荐算法</u>的概念。推荐算法是计算机专业中的一种算法，目前应用推荐算法比较好的地方主要是网络，其中淘宝做的比较好，但是作为一个老拼多多用户，我发现拼多多在这方面似乎比淘宝还要强一点。而推荐算法的原理就是利用用户的一些行为，通过一些数学算法，推测出用户可能喜欢的东西，推荐给客户。而近些年由于互联网的爆发，有了更大的数据量可以供我们使用，推荐算法才有了很大的用武之地。</p>
<h2 id="2推荐算法的条件">2.推荐算法的条件</h2>
<p>现在的各种各样的推荐算法，但是不管怎么样，都绕不开几个条件，这是推荐的基本条件<br>
1.根据和你共同喜好的人来给你推荐 。<br>
2.根据你喜欢的物品找出和它相似的来给你推荐 。<br>
3.根据你给出的关键字来给你推荐，而这一部分实际上就是一个搜索算法。<br>
4.根据上面的几种条件组合起来给你推荐。</p>
<h2 id="3推荐算法的分类">3.推荐算法的分类</h2>
<p>推荐算法实际上一共有三种分类：基于内容的推荐算法，协同过滤推荐算法，基于知识的推荐算法。</p>
<h1 id="协同过滤算法">协同过滤算法</h1>
<h2 id="1协同过滤算法理论基础">1.协同过滤算法理论基础</h2>
<p>首先我们要明确一点，在协同过滤中有两种主流算法，一是基于用户的协同过滤，二是基于物品的协同过滤。他们之间的区别在于，基于用户的协同过滤就是将那些和你具有相同喜好的用户喜欢的东西推荐给你，而基于物品的协同过滤是将和你偏好的物品具有相似特征的物品推荐给你。但是实际上，这两者的原理是相同的，就是计算用户以及物品间的相似度，他们的核心部分都是解决相似度问题。所以在阐述这两种分类之前，我们先解决他们共同的核心问题：计算相似度。</p>
<h3 id="1欧式距离的相似度测量">（1）欧式距离的相似度测量</h3>
<p><img src="https://zhangman123456.github.io/post-images/1595787357702.svg" alt="" loading="lazy">　　又称欧几里得距离，用于计算两个样本数据之间的直线距离，这说明，欧式距离更加侧重于两个数据之间的绝对距离，而不太注重这两个数据在方向上的差异</p>
<h3 id="2余弦距离衡量相似度">（2）余弦距离衡量相似度</h3>
<p>余弦相似度原理：用向量空间中的两个向量夹角的余弦值作为衡量两个个体间差异大小的度量，夹角角度越接近0°，余弦距离越接近1，也就是两个向量越相似，相似度越高；夹角角度越接近180°，余弦距离越接近0，也就是越不相似。这就叫做余弦相似。余弦相似度公式的推导比较麻烦，这里附上手写的一份。<img src="https://zhangman123456.github.io/post-images/1595841226337.jpg" alt="" loading="lazy">　　以上我写的是用于计算<strong>两个</strong>样本数据之间的余弦值（相似度），相比欧氏距离，余弦距离更加注重两个向量在方向上的差异。<br>
　　借助三维坐标系来看下欧氏距离和余弦距离的区别：<img src="https://zhangman123456.github.io/post-images/1595842641758.jpg" alt="" loading="lazy">　　从上图可以看出，欧氏距离衡量的是空间各点的绝对距离，跟各个点所在的位置坐标直接相关；而余弦距离衡量的是两个空间向量的夹角，更加体现在方向上的差异。如果保持A点位置不变，B点朝原方向远离坐标轴原点，那么这个时候两点之间的余弦距离是保持不变的，但是欧式距离会发生改变；同样的，也会有欧式距离保持不变，余弦距离发生改变的情况。这就是欧氏距离和余弦距离之间的不同之处。<br>
　　 欧氏距离和余弦距离各自有不同的计算方式和衡量特征，因此它们适用于不同的数据分析模型</p>
<h3 id="3杰卡德相似度">（3）杰卡德相似度</h3>
<h4 id="1杰卡德相似指数">①杰卡德相似指数</h4>
<p>简单来说就是计算两个集合的交集与并集的比值，这个比值结束杰卡德相似指数，用来表示两个集合之间的相似度。公式如下：<img src="https://zhangman123456.github.io/post-images/1595902222896.png" alt="" loading="lazy">不难看出，这个比值介于0~1之间。杰卡德系数越接近1，说明两个集合之间的相似度越高；杰卡德系数越接近0，说明两个集合之间的相似度越低</p>
<h4 id="2杰卡德距离">②杰卡德距离</h4>
<p>杰卡德距离与杰卡德系数刚好相反。<img src="https://zhangman123456.github.io/post-images/1595903967755.png" alt="" loading="lazy">杰卡德距离的取值也是介于0~1之间，但是杰卡德距离用于描述两个集合的不相似度，杰卡德距离越接近1，两个集合越不相似；杰卡德距离越接近0，两个集合越相似。</p>
<h2 id="2基于用户的协同过滤算法">2.基于用户的协同过滤算法</h2>
<h3 id="1算法原理">（1）算法原理</h3>
<p>它一般采用最近邻技术，利用用户的历史喜好信息计算用户之间的距离，然后利用目标用户的最近邻居用户对商品评价的加权评价值来预测目标用户对特定商品的喜好程度，从而根据这一喜好程度来对目标用户进行推荐。通俗来讲，就是将和你喜好相似的用户偏爱的东西推荐给你。</p>
<h3 id="2协同过滤的实现步骤">（2）协同过滤的实现步骤</h3>
<p>思路步骤：<br>
1.计算其他用户和目标用户的相似度（使用欧氏距离算法）；<br>
2.根据相似度的高低找出K个目标用户最相似的邻居；<br>
3.在这些邻居喜欢的物品中，根据邻居与你的远近程度算出每个物品的推荐度；<br>
4.根据每一件物品的推荐度高低给你推荐物品。</p>
<h3 id="3实例">（3）实例</h3>
<p>来看一个例子。A,B,C,D四位用户购买a,b,c,d四种商品的情况如<img src="https://zhangman123456.github.io/post-images/1595965501027.PNG" alt="" loading="lazy">也就是先把每个用户购买商品的情况转换成每个商品的购买情况，然后利用这两份数据，得出一个矩阵，这个矩阵能够大致表明哪两个用户之间是会具有兴趣相似度的，然后计算用户之间的兴趣相似度，根据兴趣相似度较高的用户购买的产品来推荐。<br>
<strong>模块化实现</strong><br>
1.建立倒排矩阵<br>
2.根据倒排矩阵计算每两个用户之间的兴趣相似度（这里采用余弦相似度）<br>
3.排列出和推荐用户兴趣相似度前k个高的用户<br>
4.根据兴趣相似度高的用户购买的产品来推荐并计算推荐度<br>
1.定义</p>
<h3 id="python代码">Python代码</h3>
<pre><code>import math
from operator import *
#例子中的数据相当于是一个用户字典{A:(a,b,d),B:(a,c),C:(b,e),D:(c,d,e)}
#我们这样存储原始输入数据
 
dic={'A':('a','b','d'),'B':('a','c'),'C':('b','e'),'D':('c','d','e')}#简单粗暴，记得加''
#定义一个函数来把用户的共同兴趣储存起来
def Usersim(dicc):#把用户-商品字典转成商品-用户字典（如图中箭头指示那样）
	item_user=dict()#这里创建一个字典，用于存放转换后的数据
	for u,items in dicc.items():#遍历我们输入的这个字典列表中的ABCD,u可以代指字典中的每个键（ABCD）或者说用户,item就是每个ABCD对应的元素
		for i in items:#遍历每一个ABCD中的每个元素。文中的例子是不带评分的，所以用的是元组而不是嵌套字典。（元组类似于列表但是元组的元素不可修改）
			if i not in item_user.keys():#如果这个元素不在新创建的这个字典的键里面（key()表示键）
				item_user[i]=set()#每一个第一次被遍历到的元素（abcd）在此之前都不会存在于这个新的字典中，但是这一行之后就存在了，这样可以将每一个键都设置为set类类型方便操作。
			item_user[i].add(u)#只要已经有了这个元素（abcd中的某一个），就向集合中添加这个元素所在的用户(ABCD)。
	C=dict()#感觉用数组更好一些，真实数据集是数字编号，但这里是字符，这边还用字典。
	N=dict()
	for item,users in item_user.items():
		for u in users:#遍历当前这个键中的每一个元素
			if u not in N.keys():#u是元素，N.key是空的字典的键，所以u是肯定不在N.key里面的，所以这一行是为了初始化
				N[u]=0   #因为这里的u遍历的是N,所以这里的作用还是初始化
			N[u]+=1 #每个商品下用户出现一次就加一次，也就是说，N是用来储存每个用户一共购买的商品个数。
            #但是这个值也可以从最开始的用户表中获得。
            #比如： for u in dic.keys():
            #             N[u]=len(dic[u])
			for v in users:
				if u==v:#这个是用于判断，是否有两个用户购买了相同的产品，相等的话，就＋1
					continue
				if (u,v) not in C.keys():#同上，没有初始值不能进行运算
					C[u,v]=0
				C[u,v]+=1  #这里还是一个字典，不过它的形状已经类似一个矩阵，它储存了所有购买重复的商品的用户的信息，以('A,B','1')的方式储存着。
#到这里倒排阵就建立好了，下面是计算相似度。这里我们采用的是余弦相似度的计算方式
	W=dict()
	for co_user,cuv in C.items():
		W[co_user]=cuv / math.sqrt(N[co_user[0]]*N[co_user[1]])#因为我不是用的嵌套字典，所以这里有细微差别。
	return W#这个W是每两个用户之间的相似度，它会把'A'和'B'之间的距离，'B'和'A'之间的距离都打印出来，尽管这两个距离相等

def Recommend(user,dicc,W2,K):
	rvi=1 #这里都是1,实际中可能每个用户就不一样了。就像每个人都喜欢beautiful girl,但有的喜欢可爱的多一些，有的喜欢御姐多一些。
	rank=dict()#定义一个字典用来储存推荐的商品和该商品的推荐度
	related_user=[]#用于储存每个用户和待推荐用户的兴趣相似度
	interacted_items=dicc[user]#储存待推荐用户已经过购买的商品
	for co_user,item in W2.items():#遍历每一对用户相似度
		if co_user[0]==user:#如果每一组用户相似度里的第一个用户等于我们所求的用户，也就是从所有数据里找出待推荐用户与每个人的兴趣相似度
			related_user.append((co_user[1],item))#先建立一个和待推荐用户兴趣相关的所有的用户列表，也就是储存每个用户和待推荐用户的兴趣相似度
	for v,wuv in sorted(related_user,key=itemgetter(1),reverse=True)[0:K]:#将每个用户和待推荐用户的兴趣相似度从大到小排列，取出前K个
	#在这里，v是在这k个与待推荐用户兴趣相似度最大的人之中遍历，而wuv就是对应的兴趣相似度
		for i in dicc[v]:#遍历&quot;与待推荐用户兴趣相似度最大的用户&quot;所购买的商品
			if i in interacted_items:#这两行是排除待推荐用户和&quot;与待推荐用户兴趣相似度最大的用户&quot;所购买的重复的商品
				continue #如果重复就跳过继续执行
			if i not in rank.keys():#除去重复商品的剩下商品就是要推荐的，在这一行以及之前肯定是还没有储存近rank里，所以这一行的作用是先初始化为零，然后再加上兴趣相似度×具体的系数
				rank[i]=0#先初始化
			rank[i]+=wuv*rvi#然后存入推荐度，也就是兴趣相似度×具体的系数
	return rank
 
if __name__=='__main__':
	W3=Usersim(dic)
	Last_Rank=Recommend('B',dic,W3,2)
    print(W3)
	print (Last_Rank)
</code></pre>
<p>首先是输出每两个用户之间的兴趣相似度：</p>
<pre><code>{('A', 'B'): 0.4082482904638631, ('B', 'A'): 0.4082482904638631, ('A', 'C'): 0.4082482904638631, ('C', 'A'): 0.4082482904638631, ('A', 'D'): 0.3333333333333333, ('D', 'A'): 0.3333333333333333, ('B', 'D'): 0.4082482904638631, ('D', 'B'): 0.4082482904638631, ('C', 'D'): 0.4082482904638631, ('D', 'C'): 0.4082482904638631}
</code></pre>
<p>然后，输入不同的用户，会给出不同的商品和商品推荐度比如输入'A'：</p>
<pre><code>{'c': 0.4082482904638631, 'e': 0.4082482904638631}
</code></pre>
<p>再比如输入b：</p>
<pre><code>{'b': 0.4082482904638631, 'd': 0.8164965809277261, 'e': 0.4082482904638631}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[K-means算法]]></title>
        <id>https://zhangman123456.github.io/post/k-means-suan-fa/</id>
        <link href="https://zhangman123456.github.io/post/k-means-suan-fa/">
        </link>
        <updated>2020-07-20T10:15:14.000Z</updated>
        <content type="html"><![CDATA[<p><strong>1.算法概述</strong>　　<br>
　　K-means，算法，简称聚类算法，通俗来讲，就是把一个区域中的所有点按照距离的远近，划分为若干个类别的算法，直观上表现为聚在一起的点分为一类。<br>
　　我们可以通过以下几张图片来直观感受一下：<br>
对于同样的一组样本数据，如果我们需要将数据分为两类，如下图：<br>
<img src="https://zhangman123456.github.io/post-images/1595435915373.jpg" alt="" loading="lazy"><br>
<strong>2.算法核心思想</strong><br>
　　K-means聚类算法是一种迭代求解的聚类分析算法，其步骤是随机选取K个对象作为初始的聚类中心（就是要把所有的数据分为ｋ个聚类），然后计算每个样本数据与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心，成为这一聚类。聚类中心以及分配给它们的对象就代表一个聚类。每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。这个过程将不断重复直到满足某个终止条件。终止条件可以是没有（或最小数目）对象被重新分配给不同的聚类，没有（或最小数目）聚类中心再发生变化，误差平方和局部最小。<br>
<strong>3. 算法实现步骤</strong><br>
　　1、首先确定一个k值，即我们希望将数据集经过聚类得到k个集合。<br>
　　2、从数据集中<u>随机选择</u>k个数据点作为质心。<br>
　　3、对数据集中每一个点，计算其与每一个质心的距离（如欧式距离），离哪个质心近，就划分到那个质心所属的集合。<br>
　　4、把所有数据归好集合后，一共有k个集合。然后重新计算每个集合的质心。<br>
　　5、如果新计算出来的质心和原来的质心之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于稳定，或者说收敛），我们可以认为聚类已经达到期望的结果，算法终止。<u>（在这里我们规定终止条件为聚类中心不再发生变化。在不同的实验中我们可以规定不同的终止条件）</u><br>
　　6、如果新质心和原质心距离变化很大，需要迭代3~5步骤。<br>
<strong>4. K-means算法优缺点</strong><br>
　　优点：<br>
　　1、原理比较简单，实现也是很容易，收敛速度快。<br>
　　2、当结果簇是密集的，而簇与簇之间区别明显时, 它的效果较好。<br>
　　3、主要需要调参的参数仅仅是簇数k。<br>
　　缺点：<br>
　　1、K值需要预先给定，很多情况下K值的估计是非常困难的。<br>
　　2、K-Means算法对初始选取的质心点是敏感的，不同的随机种子点得到的聚类结果完全不同 ，对结果　　影响很大。<br>
　　3、对噪音和异常点比较的敏感。用来检测异常值。<br>
　　4、采用迭代方法，可能只能得到局部的最优解，而无法得到全局的最优解。<br>
5.代码实现（自己的注释和理解）</p>
<pre><code>#导入numpy库
from numpy import *
#K-均值聚类辅助函数
def numpy import *#文本数据解析函数
    dataMat=[]
    fr=open(fileName)
    for line in fr.readlines():#读取文件的每一行直到结束，然后返回一个列表
        curLine=line.strip().split('\t')#以制表的分隔符为界限分开，去掉前后的空格或者换行符，制表符
        fltLine=map(float,curLine)#将每一行的数据映射成float型
        dataMat.append(fltLine)#将数据添加至
    return dataMat
   
def distEclud(vecA,vecB):
    return sqrt(sum(power(vecA-vecB,2)))#数据向量计算欧式距离 

#随机初始化K个聚类中心(满足数据边界之内)
def randCent(dataSet,k):#定义样本数据和k值为变量
    n=shape(dataSet)[1]#得到数据样本的维度
    centroids=mat(zeros((k,n)))#初始化为一个(k,n)的矩阵，每一行的元素个数都为样本数据的维度。因为我们取得的k值和样本数据的维度相同，所以在取k值时，是在每一个维度都随机取值
    for j in range(n):#遍历数据集的每一维度，计算所有k值每一维度的取值。也就是第一次循环计算所有k值的第一维度；第二次循环计算所有k值的第二维度；...一直到第n次循环计算所有k值的第n维度。
        minJ=min(dataSet[:,j]) #得到该列数据的最小值
        rangeJ=float(max(dataSet[:,j])-minJ)#得到该列数据的取值范围(最大值-最小值)
        #k个质心向量的第j维数据值随机为位于(最小值，最大值)内的某一值
        centroids[:,j]=minJ+rangeJ*random.rand(k,1)#也就是先计算所有k值在这一维度的取值。取值 = 最小值+（最大值-最小值）*（0~1）随机数
    return centroids#返回初始化得到的k个聚类中心
    
#k-均值聚类算法
#@dataSet:聚类数据集
#@k:用户指定的k个类
#@distMeas:距离计算方法，默认欧氏距离distEclud()
#@createCent:获得k个质心的方法，默认随机获取randCent()（将randCent()赋值给createCent
def kMeans(dataSet,k,distMeas=distEclud,createCent=randCent):
    m=shape(dataSet)[0]#获取数据集样本数
    clusterAssment=mat(zeros((m,2)))#初始化一个(m,2)的矩阵，第一列用于放置每一个样本数据归于哪一个聚类中心（也就是当前样本的聚类结果），第二列用于放置平方误差
    centroids=createCent(dataSet,k)#centroids = createCent() = randCent()，即引用之前创建的函数，创建初始的k个质心向量
    clusterChanged=True#聚类结果是否发生变化的布尔类型
    while clusterChanged:#只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不变化
        clusterChanged=False#聚类结果变化布尔类型置为false（也就是是说聚类结果并未发生变化）
        for i in range(m): #遍历数据集每一个样本向量
            minDist=inf;minIndex=-1#每当对一个样本数据进行距离计算时，就初始化最小距离为正无穷；最小距离对应索引为-1（也就是初始化索引为-1，为了方便以后依次叠加搜寻每一个聚类中心
            for j in range(k):#循环k个类的聚类中心
                distJI = distMeas(centroids[j,:],dataSet[i,:])#计算一个数据点到一个聚类中心的欧氏距离
                if distJI&lt;minDist:#如果距离小于当前最小距离（所以初始化最小距离为正无穷是为了方便留下第一个被计算的距离）
                    minDist=distJI;minIndex=j#直接更新当前距离为当前最小距离；最小距离对应索引对应为j(第j个聚类中心)。重复这样的迭代可以得出最小的距离
        if clusterAssment[i,0] !=minIndex:clusterChanged=True#当前聚类结果中第i个样本的聚类结果发生变化：布尔类型置为true，继续聚类算法
        clusterAssment[i,:]=minIndex,minDist**2#更新当前变化样本的聚类结果和平方误差
    print centroids#打印k-均值聚类的质心
    for cent in range(k):#遍历每一个质心
        ptsInClust=dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]#将数据集中所有属于当前聚类中心的样本通过条件过滤筛选出来
        centroids[cent,:]=mean(ptsInClust,axis=0)#计算这些数据的均值（axis=0：求列的均值），作为该类的聚类中心，也就是更新聚类中心
    return centroids,clusterAssment  #返回k个聚类，聚类结果及误差
    ```
6.由1.可以看出，k的取值对于我们分类的结果起着非常重要的作用。在应用中，k值一般来说是根据实际情况的需要，人为的确定k值。同时，我们也可以通过不停的调试K值的大小，观察不同的k值产生的分类结果以及误差来判断比较合适的k值</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[KNN算法]]></title>
        <id>https://zhangman123456.github.io/post/knn-suan-fa/</id>
        <link href="https://zhangman123456.github.io/post/knn-suan-fa/">
        </link>
        <updated>2020-07-16T08:45:34.000Z</updated>
        <content type="html"><![CDATA[<p><strong>１.算法简述</strong><br>
　　邻近算法，或者说K最近邻（KNN，K-NearestNeighbor）<u>分类算法</u>是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻近值来代表。近邻算法就是将数据集合中每一个记录进行分类的方法。<br>
<strong>２.算法详述</strong><br>
我们可以通过几幅图来形象地了解KNN算法<img src="https://zhangman123456.github.io/post-images/1594891375739.png" alt="" loading="lazy">　　毫无疑问，k值的选定对于分类的效果至关重要。如果K值过小，拟合的效果不好；如果K值过大，就会覆盖过多的样本数据，失去拟合的意义。<br>
　　要度量空间中点距离的话，有好几种度量方式，比如常见的曼哈顿距离计算，欧式距离（或欧几里得距离）等等。不过通常KNN算法中使用的是欧式距离，这里只是简单说一下，拿二维平面为例，，二维空间两个点的欧式距离计算公式如下：<img src="https://zhangman123456.github.io/post-images/1594891829868.jpg" alt="" loading="lazy">　　这样一来，不难得出，KNN算法最简单粗暴的方式就是计算预测点和所有样本点之间的距离，选出前K个距离最小的点看看哪个种类的点的数量最多。<br>
<strong>3.具体步骤</strong><br>
1.计算所有测试数据到一个训练数据之间的距离<br>
2.将这些距离按照从小到大的顺序储存起来<br>
3.取出最小的前k个点，<br>
4.确定这k个点出现的类别的频率<br>
5.返回前K个点中出现频率最高的类别作为测试数据的预测分类。<br>
6.重复步骤1-5<br>
<strong>4.KNN特点</strong><br>
KNN是一种<strong>非参</strong>的，<strong>惰性</strong>的算法模型。什么是非参，什么是惰性呢？<br>
<strong>非参</strong>的意思并不是说这个算法不需要参数，而是意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。</p>
<p><strong>惰性</strong>又是什么意思呢？想想看，同样是分类算法，逻辑回归需要先对数据进行大量训练（tranning），最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。</p>
<p><strong>KNN算法的优势和劣势</strong><br>
了解KNN算法的优势和劣势，可以帮助我们在选择学习算法的时候做出更加明智的决定。那我们就来看看KNN算法都有哪些优势以及其缺陷所在！<br>
<strong>KNN算法优点</strong><br>
简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。<br>
模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。<br>
预测效果好。<br>
对异常值不敏感<br>
<strong>KNN算法缺点</strong><br>
对内存要求较高，因为该算法存储了所有训练数据<br>
预测阶段可能很慢<br>
对不相关的功能和数据规模敏感</p>
<p>KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。因此，基于python已经有很多成熟的可以直接使用的库来进行算法的实现<br>
KNN算法的简单实现：（调用sklearn自带的库）</p>
<pre><code>from sklearn import neighbors #sklearn中的函数集
from sklearn import datasets  #sklearn中的数据集

knn = neighbors.KNeighborsClassifier() # 申明对象
iris = datasets.load_iris()  # 导入数据（一个数据集，很大的数据集）
print(iris)

knn.fit(iris.data,iris.target) # 生成KNN模型，fit()函数可以理解为一个训练的过程

predicit_label = knn.predict([[0.2,0.3,0.3,0.2]]) # 预测这个数组的概率
print(predicit_label)
</code></pre>
<p>KNN算法的具体实现：<br>
（也是copy别人的代码，自己加的注释）</p>
<pre><code>'''
导入数据
filename数据存储路径
radio，按指定比例将数据划分为训练集和测试集
trainSet：训练数据
testSet：测试数据
'''
def loadDateset(filename,radio,trainSet=[],testSet=[]):
    with open(filename,'rt') as csvfile:   #打开filename；csvfile是一种文件格式
        lines = csv.reader(csvfile)   # 逐行读取数据
        dataset = list(lines)        # 将文件中的样本数据转换为列表类型存储
        for x in range(len(dataset)-1):  # 循环每行数据，将前4个特征值存入数组
            for y in range(4):
                dataset[x][y] = float(dataset[x][y])#也就是将前四列更新成浮点数
            if random.random()&lt;radio:    # 根据radio的大小将样本数据集划分为训练数据和测试数据
                trainSet.append(dataset[x])
            else:
                testSet.append(dataset[x])
'''
计算2个样例之间的距离（欧氏距离），length表示数据的维度
'''
def evaluateDistance(instance1,instance2,length):#变量为两个样例，还有维度
    distance = 0#初始化距离
    for x in range(length): # 循环每一维度，数值相减并对其平方，然后进行累加
        distance += pow((instance1[x]-instance2[x]),2)
    return math.sqrt(distance) # 开方求距离

'''
对于一个实例，找到离他最近的k个实例
'''
def getNeighbors(trainSet,testInstance,k):
    distance = []
    length = len(testInstance)-1 # 每个测试实例的维度（比如（2,3,4）就是三个维度）
    for x in range(len(trainSet)-1): 
        dist = evaluateDistance(testInstance,trainSet[x],length)# 分别计算一个测试实例到每一个训练数据的距离
        distance.append((trainSet[x],dist)) # 将每一个训练实例和其对应到测试实例的距离存储到列表（append（）函数用于在列表末尾增添新的对象）
    distance.sort(key=operator.itemgetter(1)) #进行排序，而排序的关键是元素第二维数据的大小，也就是dist
    neighbors = [] # 用来存储离一个实例最近的几个测试数据
    for x in range(k): # 取distance中前k个实例存储到neighbors
        neighbors.append(distance[x][0])#已经经过排序的distance列表，取出前k个元素，就是一个测试实例的k个近邻
        return neighbors

'''
在最近的K个实例中投票，少数服从多数，把要预测的实例归到多数那一类
'''
def getResponse(neighbors):#neighbors作为变量
    classvotes = {} # 定义一个字典，存储每一类别的数目
    for x in range(len(neighbors)):#遍历每一个k近邻的距离值大小
        response = neighbors[x][-1]#表示第x行最后一列的元素
        if response in classvotes:
            classvotes[response] += 1
        else:
            classvotes[response] = 1
    sortedVotes = sorted(classvotes.items(),key=operator.itemgetter(1),reverse=True) # 排序，输出数目最大的类别
    return sortedVotes[0][0]

'''
计算测试集的准确率
'''
def getAccuracy(testSet,predictions):
    correct = 0
    for x in range(len(testSet)):
        if testSet[x][-1] == predictions[x]: # 每行测试用例最后一列的标签与预测标签是否相等
            correct += 1
    return (correct/float(len(testSet)))*100.0

def main():
    trainSet = [] # 存储训练集
    testSet = []  # 存储测试集
    radio = 0.80  # 按4：1划分
    loadDateset('G:/PycharmProjects/Machine_Learning/KNN/irisdata.txt',radio,trainSet,testSet) #导入数据并划分
    print(&quot;trainSetNum: &quot;+ str(len(trainSet)))
    print(&quot;testSetNum: &quot;+  str(len(testSet)))
    predictions = []
    k = 3 # 选取前k个最近的实例
    for x in range(len(testSet)): # 循环预测测试集合的每个实例
        neighbors = getNeighbors(trainSet,testSet[x],k)
        result = getResponse(neighbors)
        predictions.append(result)
        print('&gt;predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))
    accuracy = getAccuracy(testSet,predictions)
    print('Accuracy: ' + repr(accuracy) + '%')

if __name__ == '__main__':
    main()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[线性回归作业]]></title>
        <id>https://zhangman123456.github.io/post/xian-xing-hui-gui-zuo-ye/</id>
        <link href="https://zhangman123456.github.io/post/xian-xing-hui-gui-zuo-ye/">
        </link>
        <updated>2020-07-13T09:32:17.000Z</updated>
        <content type="html"><![CDATA[<p>1.该<u>样本回归方程</u>就是我我们需要拟合的曲线：<br>
                     <strong>h(x) = θ<sub>0</sub> + θ<sub>1</sub>x</strong>；<br>
2.得出关于该回归方程的误差函数:<img src="https://zhangman123456.github.io/post-images/1594575102477.gif" alt="" loading="lazy"><br>
3.对误差函数求导：<img src="https://zhangman123456.github.io/post-images/1594632959520.gif" alt="" loading="lazy"><br>
<img src="https://zhangman123456.github.io/post-images/1594575689326.gif" alt="" loading="lazy"><br>
4.之后更新的到的θ<sub>0</sub> 和 θ<sub>1</sub><img src="https://zhangman123456.github.io/post-images/1594632981086.gif" alt="" loading="lazy"><br>
<img src="https://zhangman123456.github.io/post-images/1594575337724.gif" alt="" loading="lazy">（公式里的α是学习率，我们可以象征性地理解为步长）<br>
5.接下来，利用梯度下降的方法。这里我们可以分别使用批量梯度下降和随机梯度下降的方法来实现**（有助于更加具体地理解这两种梯度下降的算法）**<br>
（1）批量梯度下降算法<img src="https://zhangman123456.github.io/post-images/1594577818861.png" alt="" loading="lazy"><u>（上标i代表具体的某一个数据；x<sub>j</sub>代表系数θ<sub>j</sub>对应的因变量）</u><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，都需要把全部的样本都使用一遍，这样重复迭代直到θ<sub>j</sub>收敛为止。所以说该算法的复杂度是O(i（j+1）)<br>
（为了防止我忘了，我写了一个一看就懂的版本）<br>
  <img src="https://zhangman123456.github.io/post-images/1594579705255.jpg" alt="" loading="lazy"><br>
（2）随机梯度下降法<img src="https://zhangman123456.github.io/post-images/1594579776426.png" alt="" loading="lazy"><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，只使用所有样本中的一个数据，一旦到达最大的迭代次数或是满足预期的精度，就停止。这样算法的复杂度为O(j+1)，就下降了很多。<br>
  <u>（还是一个一看就会的版本）</u><img src="https://zhangman123456.github.io/post-images/1594580748171.jpg" alt="" loading="lazy"></p>
<pre><code>#批量梯度下降
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
 
#数据
a = np.random.standard_normal((1, 500))
x = [150,200,250,300,350,400,600]#自变量x
y = [6450,7450,8450,9450,11450,15450,18450]#因变量y
y = y - a*10
y = y[0]
def Optimization(x,y,theta,learning_rate):
    for i in range(iter):#批量梯度下降算法的核心，iter是最大迭代的次数。
        theta = Updata(x,y,theta,learning_rate)更新参数用的函数
    return theta#这是全部迭代之后的最后一组参数值 
def Updata(x,y,theta,learning_rate):
    m = len(x)#因变量的长度，即每次迭代的次数
    sum = 0.0
    sum1 = 0.0
    alpha = learning_rate
    h = 0
    for i in range(m):#每一次迭代，都用到全部的样本数据
        h = theta[0] + theta[1] * x[i]#一元线性回归方程，即我们需要拟合的曲线。
        sum += (h - y[i])
        sum1 += (h - y[i]) * x[i]
    theta[0] -= alpha * sum / m #更新theta[0]
    theta[1] -= alpha * sum1 / m #更新theta[1]
    return theta#每一次迭代都输出更新后的参数
 
#数据初始化
learning_rate = 0.001#学习率
theta = [0,0]#最开始的参数值
iter = 1000#循环的次数
theta = Optimization(x,y,theta,learning_rate)
 
plt.rcParams['font.sans-serif']=['SimHei']#该函数用来定义图形的各种默认属性，比如字符显示，线条样式，窗口大小，坐标轴宽度等等属性
plt.rcParams['axes.unicode_minus'] = False
'''
plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('样本',fontsize=40)
plt.savefig(&quot;样本.jpg&quot;)
'''
#可视化
b = np.arange(0,50)
c = theta[0] + b * theta[1]
 
plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.plot(b,c)
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('结果',fontsize=40)
plt.savefig(&quot;结果.jpg&quot;)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[线性回归]]></title>
        <id>https://zhangman123456.github.io/post/xian-xing-hui-gui/</id>
        <link href="https://zhangman123456.github.io/post/xian-xing-hui-gui/">
        </link>
        <updated>2020-07-11T10:59:13.000Z</updated>
        <content type="html"><![CDATA[<p>1.该<u>样本回归方程</u>就是我我们需要拟合的曲线：<br>
                     <strong>h(x) = θ<sub>0</sub> + θ<sub>1</sub>x</strong>；<br>
2.得出关于该回归方程的误差函数:<img src="https://zhangman123456.github.io/post-images/1594575102477.gif" alt="" loading="lazy"><br>
3.对误差函数求导：<br>
<img src="https://zhangman123456.github.io/post-images/1594575689326.gif" alt="" loading="lazy"><br>
4.之后更新的到的θ<sub>0</sub> 和 θ<sub>1</sub><img src="https://zhangman123456.github.io/post-images/1594575337724.gif" alt="" loading="lazy">（公式里的α是学习率，我们可以象征性地理解为步长）<br>
5.接下来，利用梯度下降的方法。这里我们可以分别使用批量梯度下降和随机梯度下降的方法来实现**（有助于更加具体地理解这两种梯度下降的算法）**<br>
（1）批量梯度下降算法<img src="https://zhangman123456.github.io/post-images/1594577818861.png" alt="" loading="lazy"><u>（上标i代表具体的某一个数据；x<sub>j</sub>代表系数θ<sub>j</sub>对应的因变量）</u><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，都需要把全部的样本都使用一遍，这样重复迭代直到θ<sub>j</sub>收敛为止。所以说该算法的复杂度是O(i（j+1）)<br>
（为了防止我忘了，我写了一个一看就懂的版本）<br>
  <img src="https://zhangman123456.github.io/post-images/1594579705255.jpg" alt="" loading="lazy"><br>
（2）随机梯度下降法<img src="https://zhangman123456.github.io/post-images/1594579776426.png" alt="" loading="lazy"><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，只使用所有样本中的一个数据，一旦到达最大的迭代次数或是满足预期的精度，就停止。这样算法的复杂度为O(j+1)，就下降了很多。<br>
  <u>（还是一个一看就会的版本）</u><img src="https://zhangman123456.github.io/post-images/1594580748171.jpg" alt="" loading="lazy"></p>
<pre><code>批量梯度下降算法
@author: 
&quot;&quot;&quot;

import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
 
#数据
a = np.random.standard_normal((1, 500))
x = [150,200,250,300,350,400,600]#自变量x
y = [6450,7450,8450,9450,11450,15450,18450]#因变量y
y = y - a*10
y = y[0]
 
def Optimization(x,y,theta,learning_rate):
    for i in range(iter):#批量梯度下降算法的核心，iter是最大迭代的次数。
        theta = Updata(x,y,theta,learning_rate)更新参数用的函数
    return theta#这是全部迭代之后的最后一组参数值 
def Updata(x,y,theta,learning_rate):
    m = len(x)#因变量的长度，即每次迭代的次数
    sum = 0.0
    sum1 = 0.0
    alpha = learning_rate
    h = 0
    for i in range(m):#每一次迭代，都用到全部的样本数据
        h = theta[0] + theta[1] * x[i]#一元线性回归方程，即我们需要拟合的曲线。
        sum += (h - y[i])
        sum1 += (h - y[i]) * x[i]
    theta[0] -= alpha * sum / m #更新theta[0]
    theta[1] -= alpha * sum1 / m #更新theta[1]
    return theta#每一次迭代都输出更新后的参数
#数据初始化
learning_rate = 0.001#学习率
theta = [0,0]#最开始的参数值
iter = 1000#循环的次数
theta = Optimization(x,y,theta,learning_rate)
 
plt.rcParams['font.sans-serif']=['SimHei']#该函数用来定义图形的各种默认属性，比如字符显示，线条样式，窗口大小，坐标轴宽度等等属性
plt.rcParams['axes.unicode_minus'] = False

plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('样本',fontsize=40)
plt.savefig(&quot;样本.jpg&quot;)
'''
#可视化
b = np.arange(0,50)
c = theta[0] + b * theta[1]
 
plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.plot(b,c)
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('结果',fontsize=40)
plt.savefig(&quot;结果.jpg&quot;)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[梯度下降法]]></title>
        <id>https://zhangman123456.github.io/post/ti-du-xia-jiang-fa-yi/</id>
        <link href="https://zhangman123456.github.io/post/ti-du-xia-jiang-fa-yi/">
        </link>
        <updated>2020-07-11T01:15:53.000Z</updated>
        <content type="html"><![CDATA[<p>#基本思想<br>
假设我们爬山，如果想最快的上到山顶，那么我们应该从山势最陡的地方上山。也就是山势变化最快的地方上山<br>
同样，如果从任意一点出发，需要最快搜索到函数最大值，那么我们也应该从函数变化最快的方向搜索。<br>
函数变化最快的方向是什么呢？函数的<strong>梯度</strong>。<br>
假如函数为一元函数，<strong>梯度</strong>就是该函数的倒数<img src="https://zhangman123456.github.io/post-images/1594430603494.png" alt="" loading="lazy"><br>
如果为二元函数，梯度定义为：<img src="https://zhangman123456.github.io/post-images/1594430651348.png" alt="" loading="lazy">（此处用到高数偏导数的知识来求二元函数的倒数）<br>
如果需要找的是函数极小点，那么应该从负梯度的方向寻找，该方法称之为梯度下降法。<br>
<img src="https://zhangman123456.github.io/post-images/1594430871728.png" alt="" loading="lazy">要搜索极小值C点，在A点必须向x增加方向搜索，此时与A点梯度方向相反；在B点必须向x减小方向搜索，此时与B点梯度方向相反。总之，搜索极小值，必须向负梯度方向搜索。<br>
假设函数 y = f(x1,x2,......,xn) 只有一个极小点。初始给定参数为  X0 = (x10,x20,......xn0) 从这个点如何搜索才能找到原函数的极小值点？<br>
方法：</p>
<ol>
<li>首先设定一个较小的正数η，ε;</li>
<li>求当前位置处的各个偏导数：<img src="https://zhangman123456.github.io/post-images/1594431281163.png" alt="" loading="lazy"></li>
<li>修改当前函数的参数值，公式如下：<br>
<img src="https://zhangman123456.github.io/post-images/1594431327433.png" alt="" loading="lazy"></li>
<li>如果参数变化量小于，退出；否则返回2。<br>
例：任给一个初始出发点，设为x0=-4，利用梯度下降法求函数y=x²/2-2x的极小值。<br>
准备工作：<br>
（1）给定两个参数值η = 0.9，ε = 0.01（η即为学习效率，或者步长）<br>
（2）计算导数：dy/dx = x - 2;<br>
一.（1）计算当前导数值：y' = -4 - 2 = -6；<br>
   （2）修改当前的参数值：x = x-ηy' = -4 - 0.9*(-6) = 1.4;<br>
  （3）计算△x = -0.9 * （-6）= 5.4。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环<br>
二.（1）计算当前导数值：y' = 1.4 - 2 = -0.6；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.4 - 0.9*(-0.6)= 1.94;<br>
  （3）计算△x = -0.9 * （-0.6）= 0.54。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环。<br>
三.（1）计算当前导数值：y' = 1.94 - 2 = -0.06；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.94 - 0.9*(-0.06)= 1.994;<br>
  （3）计算△x = -0.9 * （-0.06）= 0.054。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环。<br>
四.（1）计算当前导数值：y' = 1.994 - 2 = -0.006；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.994 - 0.9*(-0.006)= 1.9994;<br>
  （3）计算△x = -0.9 * （-0.006）= 0.0054。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&lt;=ε，则终止循环，输出此时的x即为函数的极小点。<br>
python代码实现</li>
</ol>
<pre><code class="language-python">＃解决实例的Python代码
import numpy as np
inport matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams['font.family'] = 'sans-serif'
mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = false

#构建一维元素图形
def f1(x):
　　return 0.5 * (x ** 2) - 2x
def h1(x):
　　return x-2

#使用梯度下降法
GD_X = []
GD_Y = []
x = -4
alpha = 0.9
f_change = np.abs(alpha * hl(x))
f_current = f_change
GD_X.append(x)
GD_Y.append(f_current)

#迭代次数
iter_num = 0
while f_change &gt;0.01 and iter_num&lt;10:
　　iter_num+=1
　　x = x - x - alpha * h1(x)
　　tem = f1(x)
　　f_change = np.abs(alpha * hl(x))
　　f_current = tmp
　　GD_X.append(f_current)

print(u'最终的结果：(%.5f,%.5f)'%(x,f_current))
pirnt(u'迭代次数：%d'%iter_num)
printf(GD_X)
</code></pre>
<p>#损失函数<br>
学习了梯度下降之后，我们引入新的概念：<strong>损失函数</strong>。便于我们直接利用梯度下降法计算损失函数的最小值。<br>
我们可以近似地将<strong>损失函数</strong>理解为方差或者平方差。根据给出的一系类数据我们可以得出相关的函数，再根据该函数和不同变量，我们可以得出不同变量对应的不同的函数值。而每个函数值和每个真实的数值之间有一定的误差，根据这些误差我们可以得到相关的函数，即<strong>损失函数</strong>。<strong>损失函数</strong>的值越小，我们之前得到的函数就越精确。<br>
例：根据某组数据得到预测的函数：h(x) = θ₁ + θ₂x。只含有一个特征/输入变量。在这个式子中θ₁，θ₂都是未知参数，x是已知数据。我们的目的是得到合适的参数使得我们的预测尽可能准确。这时就需要引入损失函数的概念。<br>
<img src="https://zhangman123456.github.io/post-images/1594383086368.jpg" alt="" loading="lazy"><em>(暂时不用管1/2)</em><br>
上图中的J(θ₁,θ₂)是一个关于θ₁和θ₂的二元函数，也就是<strong>损失函数</strong>。在三维空间坐标系内，存在一对点（θ₁,θ₂）使得J(θ₁,θ₂)有最小值<br>
然后对损失函数进行一个简单的推导（利用梯度下降的算法）<br>
<img src="https://zhangman123456.github.io/post-images/1594444209052.jpg" alt="" loading="lazy">（在这个公式里突然多了一个α，它表示的是学习率，用来限定步长的大小，也很容易理解，毕竟得到的偏导是类似斜率的东西，只是一个方向，总得加个数值，才能表示向这个方向移动的距离。<br>
对于的取值，一般都取的比较小，但也不要太小，太小就意味着迭代步数要增加，运算时间边长。另外，迭代的步数要自己设置。<br>
原文链接：https://blog.csdn.net/i96jie/java/article/details/81206473）<br>
#梯度下降法的三个变种<br>
了解了梯度下降法的核心思想以及损失函数后，我们接着了解梯度下降法的三个变种<br>
1.批量梯度下降法（BGD）<br>
其需要计算整个训练集的梯度，即：<img src="https://i.loli.net/2019/08/12/zAiXFLUSyrd5qap.png" alt="" loading="lazy">其中η为学习率，用来控制更新的“力度”/&quot;步长&quot;。<br>
优点：<br>
    对于凸目标函数，可以保证全局最优； 对于非凸目标函数，可以保证一个局部最优。<br>
2.随机梯度下降算法（SGD）<br>
仅计算某个样本的梯度，即针对某一个训练样本 xi及其label yi更新参数：<img src="https://i.loli.net/2019/08/12/ItgcSRaT4jreKMl.png" alt="" loading="lazy">逐步减小学习率，SGD表现得同BGD很相似，最后都可以有不错的收敛。<br>
优点：<br>
    更新频次快，优化速度更快; 可以在线优化(可以无法处理动态产生的新样本)；一定的随机性导致有几率跳出局部最优(随机性来自于用一个样本的梯度去代替整体样本的梯度)。<br>
3.小批量梯度下降算法(MBGD),计算包含n个样本的mini-batch的梯度：<img src="https://i.loli.net/2019/08/12/ItgcSRaT4jreKMl.png" alt="" loading="lazy">MBGD是训练神经网络最常用的优化方法。<br>
优点：<br>
参数更新时的动荡变小，收敛过程更稳定，降低收敛难度；可以利用现有的线性代数库高效的计算多个样本的梯度。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://zhangman123456.github.io/post/hello-gridea/</id>
        <link href="https://zhangman123456.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>