<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://zhangman123456.github.io</id>
    <title>张曼</title>
    <updated>2020-09-26T21:30:40.250Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://zhangman123456.github.io"/>
    <link rel="self" href="https://zhangman123456.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://zhangman123456.github.io/images/avatar.png</logo>
    <icon>https://zhangman123456.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 张曼</rights>
    <entry>
        <title type="html"><![CDATA[GCN（图卷积神经网络）]]></title>
        <id>https://zhangman123456.github.io/post/gcntu-juan-ji-shen-jing-wang-luo/</id>
        <link href="https://zhangman123456.github.io/post/gcntu-juan-ji-shen-jing-wang-luo/">
        </link>
        <updated>2020-09-22T08:24:12.000Z</updated>
        <content type="html"><![CDATA[<p>前面我们已经学习过了卷积神经网络CNN，其实GCN就是CNN在图上面的应用。在CNN中我们输入的是一张图片，然而随着CNN的广泛应用，我们发现它并不能满足所有的应用。比如，我们对一个群聊内所有人的关系进行分析时，那么这个关系网就是一个图。每个人作为一个结点，都有若干个不确定的人和他关联。在这种情况下，CNN就不能很好地发挥作用，于是在CNN的基础上，我们提出了应用于图的卷积神经网络GCN。<br>
　　提示：在对GCN进行理解时，可以将GCN的一些部分类比于CNN来学习，对于已经掌握CNN的同学来说可能会比较好理解。</p>
<h1 id="一些数学基础">一些数学基础</h1>
<p>在这之前，我们需要重温一些基础的高数和离散数学的知识。由于本人数学废柴，而且记性比较差，所以很多简单的定义会写的比较白话，方便我自己能看懂，数学比较好的童鞋可以自行跳过<br>
　　图的定义：图分为有向图和无向图，都可以用G=(V，E)来表示，V表示结点的集合，E表示有向或者无向边的集合。<br>
　　邻接矩阵：分为有向图邻接矩阵和无向图邻接矩阵，矩阵的大小为 N×N ，N为图的结点数，矩阵内的元素表示一个顶点和另一个顶点之间的关系（用0和1来表示无或者有），且无向图的邻接矩阵一定是关于对角线对称的。但是有向图的邻接矩阵不一定。<br>
<img src="https://zhangman123456.github.io/post-images/1600778653916.webp" alt="" loading="lazy"><br>
　　特征矩阵：设 A 是n阶方阵，如果存在数m和非零n维列向量 x，使得 Ax=mx 成立，则称 m 是矩阵A的一个特征值或本征值（如图）<br>
<img src="https://zhangman123456.github.io/post-images/1600776572242.png" alt="" loading="lazy"><br>
　　GCN里面的特征矩阵可以类比CNN里面的filter，都是为了提取特征用的<br>
　　度矩阵：度矩阵是对角阵，对角上的元素为各个顶点的度。顶点vi的度表示和该顶点相关联的边的数量。<br>
　　无向图中顶点vi的度d(vi)=N(i)，且除了对角线以外的所有元素值都为零。<br>
<img src="https://zhangman123456.github.io/post-images/1600780770635.webp" alt="" loading="lazy"><br>
　　拉普拉斯矩阵：给定一个有n个顶点的图G，它的拉普拉斯矩阵：<br>
<img src="https://zhangman123456.github.io/post-images/1600781035629.webp" alt="" loading="lazy"><br>
　　定义为:L=D-A。其中D为图的度矩阵，A为图的邻接矩阵。这样用公式可能有点抽象，我们可以用一个例子来对这个定义进行解释：<br>
<img src="https://zhangman123456.github.io/post-images/1600781229978.webp" alt="" loading="lazy"><br>
　　</p>
<h1 id="gcn的运行机制">GCN的运行机制</h1>
<p>GCN是一个在图上进行操作的神经网络。给定一个图 G=(E,V) ,一个 GCN 的输入包括：<br>
　　 一个 N×N 的对于图结构的表示的矩阵，例如 G 的邻接矩阵 A。这个邻接矩阵实际上就是我们根据实际情况输入的一个结构图的邻接矩阵。比如，一个群聊里面所有人的人际关系，把人作为结点，两个人之间是否认识作为两个结点之间的关系，就可以构成一个图G，A就是这个图G的邻接矩阵。这个邻接矩阵可以类比为CNN里面输入的图像的三维矩阵。<br>
　　 一个输入特征矩阵 X，其维度是 N×F ,其中 N 是节点的数目（即Ｅ），F 是每个节点输入特征的数目。这个特征矩阵是用来对输入的邻接矩阵进行特征提取的。可以理解为，特征矩阵的每一行都对应于一个结点的F个特征数目。<br>
　　GCN 的一个隐藏层可以写成 Hi=f(Hi-1,A),其中 H0=X 并且 f 是一个 propagation。每 层Hi 对应一个 N×Fi 的特征矩阵，矩阵的每行是一个节点的特征表示。在每层，这些特征 通过 propagation rule f 聚合起来形成下一层的特征。通过这种方式，特征变得越来越抽 象在每一层。在这个框架中，GCN 的各种变体仅仅是在 propagation rule f 的选择上有 不同。<br>
　　一个简单的 Propagation Rule 一个可能最简单的传播规则是： f(Hi,A)=σ(AHiWi) 其中 Wi 是第 i 层的权重并且σ是一个非线性激活函数例如 ReLU 函数。权重矩阵的维度是 Fi×Fi+1；也就是说权重矩阵的第二个维度决定了在下一层的特征的数目。如果你对卷积神 经网络熟悉，这个操作类似于 filtering operation 因为这些权重被图上节点共享。 可以简单表示为：<br>
<img src="https://zhangman123456.github.io/post-images/1600857540043.PNG" alt="" loading="lazy"><br>
　　说实话，关于GCN的隐藏层计算这段话很难理解，这个公式也很难懂，所以很难真正看明白GCN的隐藏层在进行运算时究竟是如何计算的，计算的结果到底收到什么因素的影响，而且我在网上能查阅的资料中也基本上都是把这段话复制粘贴一下，没有浅显易懂的理解（至少对于我来说是这样的）。所以我们可以举一个简单的例子，然后再回过头来看这段话，理解起来就会容易得多。</p>
<h1 id="一个简单的例子">一个简单的例子</h1>
<p>首先来看这个有向图<br>
<img src="https://zhangman123456.github.io/post-images/1600858003893.PNG" alt="" loading="lazy"><br>
这个图的邻接矩阵表示是：（为了方便，直接用代码表示，后期可以直接运行证明）<br>
import numpy as np<br>
A = np.matrix([<br>
　　　　　　　 [0, 1, 0, 0],<br>
　　　　　　 　[0, 0, 1, 1],<br>
　　　　　　　 [0, 1, 0, 0],<br>
　　　　　　　 [1, 0, 1, 0]],<br>
　　　　　　　dtype=float )<br>
　　X 为输入的特征向量，我们直接取值，维度为(N,F0),其中 N 为结点个数，F0为输入向量的特征维数。接下来，我们需要特征！我们根据每个节点的索引来生成两个整数特征。这会使得后面手动 验证矩阵的计算过程更容易。<br>
X = np.matrix([<br>
　　　　　　　[i, -i]<br>
　　　　　　　for i in range(A.shape[0])<br>
　　　　　　　 ],<br>
　　　　　　　dtype=float)<br>
X = ([<br>
　　 [0, 0],<br>
　　 [1,-1],<br>
　　 [2,-2],<br>
　　 [3,-3],<br>
　　])<br>
所以Input = A*X = ([<br>
[1,-1],<br>
[5,-5],<br>
[1,-1],<br>
[2,-2],<br>
])<br>
　　我们会发现，输入的特征矩阵和相乘后输出的矩阵维度都是4*2，都可以理解为每一行都是一个结点的F个特征数。而在相乘的过程中，我们不难发现，每一个元素的每一个特征值，都是由该元素的邻居的特征值之和组成的。我们不妨分解的再详细一点：<br>
Input[0,0] = 0×0 + 1×1 + 0×2 + 0×3 = 1<br>
　　Input[0,0]对应的是第一个结点的第一个特征的计算值，这个值的计算结果是由它的邻居的特征值之和计算得来的。</p>
<h1 id="使用-propagation-rule-即将出现的问题">使用 Propagation Rule 即将出现的问题</h1>
<p>在这个运行机制中，我们很容易发现一些问题<br>
　　 <font color="#dd0000">A*X 的结点表示中，并没有加自己的特征值。</font><br />　　一个节点的聚集表示不包括它自己 的特征，这个表示只是它的邻居节点特征的聚集，所以只有有一个自循环的节点才会 包括它自己的特征在聚集中。<br>
　　 <font color="#dd0000">邻接结点多的结点的特征值会大，少的特征值就小。</font><br />　　具有很大度数的节点将会有 很大的值在它们的特征表示中，而具有很小度数的节点将会有很小的值。这些可能造成梯度消失或者梯度爆炸。这对于随机梯度下降也可能是有问题的，随机梯度下降通 常被用来训练这样的网络，而且对于每个输入特征的值的范围是敏感的。</p>
<h1 id="解决方法">解决方法</h1>
<p>为了解决第一个问题，我们可以通过简单地给每个节点加入一个自循环（self-loops）。在实践中，这可以通过把单位矩阵 I 加到邻接矩阵 A （在应用传播规则之前）上来实现。<br>
　　为了解决第二个问题，我们使用归一化的技巧。<br>
　　将自循环和归一化的技巧结合起来，并且引入权重参数和激活函数，一个图卷积神经网络的大致流程就出来了，我们依然取上面的计算作为例子<br>
1，取权重参数: W = np.matrix([ [1, -1],<br>
　　　　　　　　　　　　　　　　　　　[-1, 1] ])<br>
　　取特征矩阵：Ｘ = <img src="https://zhangman123456.github.io/post-images/1601151485736.PNG" alt="" loading="lazy"><br>
2，自循环处理：<br>
（1）设置单位矩阵I：I = np.matrix(np.eye(A.shape[0]))<br>
<img src="https://zhangman123456.github.io/post-images/1601146607050.PNG" alt="" loading="lazy"><br>
（2）将单位矩阵和邻接矩阵A相加：A_hat = A + I<br>
　　　　　　　　　　　　　　　　A_hat = ([[1, 1, 0, 0],<br>
　　　　　　 　　　　　　　　　　　　　　[0, 1, 1, 1],<br>
　　　　　　　 　　　　　　　　　　　　　[0, 1, 1, 0],<br>
　　　　　　　 　　　　　　　　　　　　　[1, 0, 1, 1]],<br>
　　　　　　　　　　　　　　　　　　　　dtype=float )<br>
3，归一化处理：<br>
（1）首先计算图A_hat的度矩阵D_hat（这里应该是计算出度和入度都可以的）<br>
　　　D_hat = np.array(np.sum(A_hat,axis=0))[0] ＃计算矩阵A的每个节点的入度存放在一个数组中<br>
　　　D_hat = np.matrix(np.diag(D_hat)) # 变成一个完整的度矩阵<br>
　　　D_hat =<br>
<img src="https://zhangman123456.github.io/post-images/1601151154016.PNG" alt="" loading="lazy">　　　　　<br>
（2）转换邻接矩阵：取度矩阵的负一次方（D_hat**-1）乘上 A_hat。<br>
（3）D_hat**-1 * A_hat * X * W = <img src="https://zhangman123456.github.io/post-images/1601151645949.PNG" alt="" loading="lazy"><br>
注：在计算过程中，输出的特征表示的维度（也就是特征数目）是由输入的特征矩阵X的列数决定的。如果我们想要减少输出特征表示的维度，我们可以减少权重矩阵的规模。</p>
<h1 id="加入激活函数">加入激活函数</h1>
<p>我们选择保持特征表示的维度并且应用 ReLU 激活函数。<br>
　　　　relu(D_hat**-1 * A_hat * X * W)<br>
　　　　matrix([[1., 0.],<br>
　　　　　　　　[4., 0.],<br>
　　　　　　　　[2., 0.],<br>
　　　　　　　　[5., 0.]])<br>
这就是一个带有邻接矩阵，输入特征，权重和激活函数的完整的隐藏层。</p>
<h1 id="从图的结构来理解">从图的结构来理解</h1>
<p>在计算机的程序运行中，我们是把图转化成了矩阵来进行处理，同时我们也可以从图的结构的角度来简单理解一下图卷积计算的过程。过程比较不太详细，但是更加直观，有助于形象地理图卷积网络。<br>
<img src="https://zhangman123456.github.io/post-images/1601153682265.jpg" alt="" loading="lazy"><br>
这个图卷积神经网络的第一层是输入的图，在第二层Hidden layer中，每一个结点都和特征矩阵进行计算，每一个计算出来的结果都会在节点处被更新，再经过一层Relu激活函数之后，进入新的隐藏层，重新进行计算。</p>
<p></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNN卷积神经网络]]></title>
        <id>https://zhangman123456.github.io/post/cnn-juan-ji-shen-jing-wang-luo/</id>
        <link href="https://zhangman123456.github.io/post/cnn-juan-ji-shen-jing-wang-luo/">
        </link>
        <updated>2020-08-08T02:31:26.000Z</updated>
        <content type="html"><![CDATA[<p>CNN卷积神经网络是DNN人工神经网络中最成功的一类。</p>
<h1 id="cnn的基本架构">CNN的基本架构<img src="https://zhangman123456.github.io/post-images/1596882654558.PNG" alt="" loading="lazy"></h1>
<p>这个模型看起来有些庞大，也很复杂，但是我们可以一层一层地来拆解它。</p>
<h1 id="卷积层">卷积层</h1>
<p>卷积层是对输入的数据进行一个卷积操作，我们可以通过一个动态图来理解：<a href="https://cs231n.github.io/assets/conv-demo/index.html">https://cs231n.github.io/assets/conv-demo/index.html</a><br>
这个动态图也许有些难以理解，但是没关系，我们可以把它分成几帧来处理。而在具体讲解之前，我们先来解释几个专业术语。<br>
1.Input Volume：指的是输入后经过填充大的三维矩阵。由于CNN主要应用于图像的识别和处理，所以我们可以象征性地把这个矩阵的长和宽理解为输入的图片的长和宽，把矩阵的深度理解为这个图片的特征。比如gdb图片，其实就是指这个图片上的每一个像素点都是由三个特征g,b,d组成的。<br>
2.Filter：可 以近似的理解为人工神经网络中的权重参数，只不过人工神经网络中的参数往往是一个固定的数值，而CNN中的参数往往是一个我们自己定义的三维矩阵。<br>
3.Bias：设置的偏置值。可以近似的理解为神经网络中的偏置值。<br>
4.stride：步长。每计算出一个值，矩阵向右或者向下移动的距离。<br>
5.padding：填充。在我们计算时，习惯在输入的矩阵的周围加上若干圈数值为零的行和列，方便后面的计算。填充的值就是这个加上的行或者列数。<br>
6.Output Volume：经过一次卷积计算后输出的矩阵。<br>
第一次计算：<img src="https://zhangman123456.github.io/post-images/1597029569759.PNG" alt="" loading="lazy"><br>
图中我们不难看出，Filter的深度必须和Input Volume的深度相同。把图中这三对用黑色线相连的矩阵相乘，然后把得出的三个数值相加，在加上偏置值就是Output Volume第一个矩阵的第一个值。<br>
如图，第一行黑线相连的两个矩阵，相乘是这样计算：<br>
　　　　０×１＋０×１＋０×１＋０× -1＋１× -1＋１×０＋０× -1＋０×１＋１×０＝ -2<br>
每一对矩阵都是这样计算，计算完之后相加然后加上偏置值，就是最右边第四列Output Volume的第一个元素的值。<br>
接下来，根据我们stride的长度，将每一层的矩阵向右平移两个单位，然后重复第一次计算的操作，将计算出来的值填入Output Volume，作为第二个元素的值。<br>
<img src="https://zhangman123456.github.io/post-images/1597327648096.PNG" alt="" loading="lazy"><br>
不难看出，接下来的计算也是这样，将第一次平移后的矩阵再平移一次，重复第一次的操作，将计算出来的值填入Output Volume作为第三个元素的值<br>
<img src="https://zhangman123456.github.io/post-images/1597067686341.PNG" alt="" loading="lazy"></p>
<p>根据这三帧图像，我们可以大致总结卷积计算中矩阵移动的规律：<br>
<img src="https://zhangman123456.github.io/post-images/1597029547398.gif" alt="" loading="lazy">根据这样的移动规律，我们可以将整个矩阵针对第一个Filter进行计算。并以此类推，根据一样的方式对第二个FIlter进行计算，得出的结果填入Output Volume的第二个矩阵。</p>
<h2 id="padding">padding</h2>
<p>在这个栗子中，padding = 1，那么在卷积计算中，为什么要用padding呢？从卷积神经网络的移动方式我们不难看出，依照卷积神经网络的计算方式，越靠近矩阵中间的数据，被计算的次数越多，也就意味着对于结果矩阵的影响越大，越靠近矩阵边缘的数据，被计算的次数就越少，比如四个角落的数据，被计算用到的次数只有一次。而这显然是不公平的，所以我们可以在Input Volume的周围加上一圈对计算结果不造成影响的0作为行和列，使矩阵中的每一个元素都能得到充分的计算，这就是padding的作用。</p>
<h1 id="池化层">池化层</h1>
<p>相较于卷积层，池化层则简单得多。池化层的操作主要是将输入矩阵的长和宽缩小一定的倍数，但是矩阵的深度不发生改变，也就是矩阵的特征数不发生改变。我们可以来看一个栗子<img src="https://zhangman123456.github.io/post-images/1597327194631.PNG" alt="" loading="lazy"><br>
这个三维矩阵很大，我们通过缩减长和宽，比如将长和宽缩减到原来的1/2，这样矩阵的体积就缩减到了原来的1/4。<br>
接下来我们来看池化的方式。既然是缩减到原来的1/4，方式就是以一个2×2的矩阵为一个单位，从每一个矩阵中选取一个数字来代表这个矩阵。我们可以形象地理解为，降低一张图片的像素。<img src="https://zhangman123456.github.io/post-images/1597327243704.PNG" alt="" loading="lazy"><br>
接下来我们来看几个数据问题：<br>
我们设输入的<br>
<img src="https://zhangman123456.github.io/post-images/1597138688806.PNG" alt="" loading="lazy"></p>
<p>用numpy来实现离散卷积</p>
<pre><code>import numpy as np
import h5py
#import matplotlib.pyplot as plt

#%load_ext autoreload
#%autoreload 2

np.random.seed(1) # 指定随机数种子
def zero_pad(X, pad):#把输入的矩阵进行填充
    &quot;&quot;&quot;
    把数据集X的图像边界用0值填充。填充情况发生在每张图像的宽度和高度上。
    
    参数:
    X -- 图像数据集 (m, n_H, n_W, n_C)，分别表示样本数、图像高度、图像宽度、通道数 
    pad -- 整数，每个图像在垂直和水平方向上的填充量
    
    返回:
    X_pad -- 填充后的图像数据集 (m, n_H + 2*pad, n_W + 2*pad, n_C)
    &quot;&quot;&quot;
    # X数据集有4个维度，填充发生在第2个维度和第三个维度上；填充方式为0值填充
    X_pad = np.pad(X, (
        (0, 0),# 样本数维度，不填充
        (pad, pad), #n_H维度，上下各填充pad个像素
        (pad, pad)), #n_C维度，不填充
                 mode='constant', constant_values = (0, 0))
    
    return X_pad

def numpy_conv(inputs,filter,_result,padding=&quot;VALID&quot;):#计算每一个filter对应的结果矩阵的值
    H, W = inputs.shape
    filter_size = filter.shape[0]
    # default np.floor
    filter_center = int(filter_size / 2.0)
    filter_center_ceil = int(np.ceil(filter_size / 2.0))

    #这里先定义一个和输入一样的大空间，但是周围一圈后面会截掉
    result = np.zeros((_result.shape))
    #更新下新输入,SAME模式下，会改变HW
    H, W = inputs.shape
    #print(&quot;new size&quot;,H,W)
    #卷积核通过输入的每块区域，stride=1，注意输出坐标起始位置
    for r in range(0, 3):
        for c in range(0, 3):
            cur_input = inputs[2*r:2*r + filter_size,
                        2*c:2*c + filter_size]# 池化大小的输入区域
            cur_output = cur_input * filter#和核进行乘法计算
            conv_sum = np.sum(cur_output) #再把所有值求和
            result[r, c] = conv_sum #当前点输出值
    return result

def _conv(inputs, filter,strides=[1,1], bias = [1,0], padding=&quot;VALTD&quot;):
    C_in, H, W = inputs.shape
    filter_size = filter.shape[2]
    # C_out指核对个数，也是最后结果对通道个数
    C_out = filter.shape[0]
    # 同样我们任务核对宽高相等
    result = np.zeros([C_out, int(np.ceil(H - filter_size + 2) /2), int(np.ceil(W - filter_size + 2) /2)],np.float32) 
    # 核个数对循环
    for channel_out in range(C_out):#遍历每一个filter矩阵
        for channel_in in range(C_in):#遍历输入矩阵的每一层（也可以说是遍历filter的每一层） # 当前通道对数据
            channel_data = inputs[channel_in]
            if channel_out == 0:
                result[channel_out, :, :] += numpy_conv(channel_data, filter[channel_out][channel_in], result[0],padding)+1
            else:
                result[channel_out, :, :] += numpy_conv(channel_data, filter[channel_out][channel_in], result[0],padding)

    # print(result)
    return result

if __name__ == '__main__':
    #输入[C_in,H,W]
    X = np.array([[[0,1,1,0,2],[2,2,2,2,1],[1,0,0,2,0],[0,1,1,0,0],[1,2,0,0,2]],[[1,0,2,2,0],[0,0,0,2,0],[1,2,1,2,1],[1,0,0,0,0],[1,2,1,1,1]],[[2,1,2,0,0],[1,0,0,1,0],[0,2,1,0,1],[0,1,2,2,2],[2,1,0,0,1]]])
    inputs = zero_pad(X,1)

    print(&quot;input:\n&quot;,inputs,&quot;\n&quot;)

    #卷积核[C_out,C_in,K,K]
    filter = np.zeros([2, 3, 3, 3])
    filter[0] = np.array([[[-1,1,0],[0,1,0],[0,1,1]],[[-1,-1,0],[0,0,0],[0,-1,0]],[[0,0,-1],[0,1,0],[1,-1,-1]]])
    filter[1] = np.array([[[1,1,-1],[-1,-1,1],[0,-1,1]],[[0,1,0],[-1,0,-1],[-1,1,0]],[[-1,0,0],[-1,0,1],[-1,0,0]]])

    print(&quot;filter\n&quot;,filter,&quot;\n&quot;)
    
    final_result = _conv(inputs, filter, strides=[1,1],bias = [1,0], padding = &quot;VALID&quot;)

    print(&quot;result\n&quot;,final_result,&quot;\n&quot;)
</code></pre>
<p>我的博客即将同步至腾讯云+社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=15i4llkq69fhl</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[矩阵分解]]></title>
        <id>https://zhangman123456.github.io/post/ju-zhen-fen-jie/</id>
        <link href="https://zhangman123456.github.io/post/ju-zhen-fen-jie/">
        </link>
        <updated>2020-07-28T20:08:42.000Z</updated>
        <content type="html"><![CDATA[<p><strong>矩阵分解的概念</strong><br>
　　矩阵分解是指将一个矩阵分解成两个或者多个矩阵的乘积。譬如，一个N行M列的矩阵可以被分解为一个N行K列的矩阵和一个K行M列的矩阵的乘积（注意乘积顺序不能颠倒）。K可以理解为一个参数，这个参数会对这两个矩阵乘积产生的矩阵大小产生影响，但是不会影响乘积得到的矩阵的行数和列数。可以利用矩阵分解来解决很多实际生活中的问题。<br>
<strong>例：</strong><br>
看图中的这个例子：<img src="https://zhangman123456.github.io/post-images/1596021541880.png" alt="" loading="lazy">　　矩阵中，描述了5个用户(U1,U2,U3,U4 ,U5)对4个物品(D1,D2,D3,D4)的评分(1-5分)，- 表示没有评分，我们的目的是把没有评分的给预测出来，然后按预测的分数高低，给用户进行物品推荐。<br>
　　将图中的数据看作是一个维度为N×Ｍ的评分矩阵R，则R可以看做两个矩阵P(N×Ｋ)和Ｑ(Ｋ×Ｍ)的乘积。对于P,Q矩阵的解释，直观上，P矩阵是N个用户对K个主题的关系，Q矩阵是K个主题跟M个物品的关系，或者说，N个用户通过K个主题对M个物品进行评分，那么我们在模拟时，也要通过这K个主题进行模拟。至于K个主题具体是什么，在算法里面K是一个参数，需要调节的，通常10~100之间。<br>
具体步骤：<br>
　　矩阵的分解（R是真实的评分矩阵，R^是我们预测得到的矩阵。R与R^越接近，就证明我们取得两个矩阵P和Q的预测模型越好。N和M是确定的参数值不会改变，因此K的取值对整个模型的预测结果起着至关重要的作用)<img src="https://zhangman123456.github.io/post-images/1596076385604.png" alt="" loading="lazy"><br>
　　另外在这里补充说明一下矩阵相乘的规则：Ｒ＾矩阵的第ｉ行第ｊ列的元素 = P 矩阵第ｉ行的元素×Ｑ矩阵第ｊ列的元素。注意P和Q不能颠倒。<img src="https://zhangman123456.github.io/post-images/1596076633042.png" alt="" loading="lazy"><br>
　　损失函数的平方等于真实值与预测值之间差值的平方，可以直接把预测值替换成P和Q的乘积。所以损失函数的值越小，真实值与预测值之间的差距越小，也就意味着我们拟合的结果越接近真实值，也就是我们对矩阵分解的结果越好。所以接下来就是解决问题的核心：基于梯度下降的优化算法。利用梯度下降的方法，一步步逼近损失函数的最小值<img src="https://zhangman123456.github.io/post-images/1596076674246.jpg" alt="" loading="lazy"><br>
　　但是在机器学习中，我们通常在损失函数的后面加上一个正则项，公式如下：<img src="https://zhangman123456.github.io/post-images/1596076704081.png" alt="" loading="lazy"><br>
　　这样一来，我们也需要对算法做出一点改动<img src="https://zhangman123456.github.io/post-images/1596076729403.jpg" alt="" loading="lazy"><br>
<strong>代码实现</strong></p>
<pre><code>import numpy as np  
import matplotlib.pyplot as plt
 
 
def matrix(R, P, Q, K, alpha, beta):
    result=[]
    steps = 1
    while 1 :
    #使用梯度下降的一步步的更新P,Q矩阵直至得到最终收敛值
        steps = steps + 1    
        eR = np.dot(P,Q)# dot(P,Q) 表示矩阵内积,即Pik和Qkj，
        e=0
        for i in range(len(R)):#len(R)代表的是R的行数
            for j in range(len(R[i])):#这两行就是遍历R中的每一个元素
                if R[i][j]&gt;0:
                    #k由1到k的和，eij为真实值和预测值的之间的误差,
                    eij=R[i][j]-.dot(P[i,:],Q[:,j]) 
                    e=e+pow(R[i][j] - np.dot(P[i,:],Q[:,j]),2) #求误差函数值，我们在下面更新p和q矩阵的时候我们使用的是化简得到的最简式，较为简便，但下面我们仍久求误差函数值，这里e求的是每次迭代的误差函数值，用于绘制误差函数变化图
                    for k in range(K):#在上面的误差函数中加入正则化项防止过拟合
                        e=e+(beta/2)*(pow(P[i][k],2)+pow(Q[k][j],2))     
                    for k in range(K):
                        #在更新p,q时我们使用化简得到了最简公式
                        P[i][k]=P[i][k]+alpha*(2*eij*Q[k][j]-beta*P[i][k])
                        Q[k][j]=Q[k][j]+alpha*(2*eij*P[i][k]-beta*Q[k][j])
        #print('迭代轮次:', steps, '   e:', e)
        result.append(e)#将每一轮更新的损失函数值添加到数组result末尾
        
        #当损失函数小于一定值时，迭代结束
        if eij&lt;0.00001:
            break
    return P,Q,result
   
R=[
   [5,3,0,1],
   [4,0,0,1],
   [1,1,0,5],
   [1,0,0,4],
   [0,1,5,4],
   ]
 
R=np.array(R)
    
alpha = 0.0001 #学习率
beta = 0.002 
 
N = len(R) #表示行数
M = len(R[0]) #表示列数
K = 3 #3个因子
 
p = np.random.rand(N, K) #随机生成一个 N行 K列的矩阵
q = np.random.rand(K, M) #随机生成一个 M行 K列的矩阵
 
P, Q, result=matrix(R, p, q, K,  alpha, beta)
print(&quot;矩阵R为：\n&quot;,R)
print(&quot;矩阵P为：\n&quot;,P)
print(&quot;矩阵Q为：\n&quot;,Q)
MF = np.dot(P,Q)
print(&quot;预测矩阵：\n&quot;,MF)
print()
#下面代码可以绘制损失函数的收敛曲线图
print(&quot;损失函数的收敛过程为：&quot;)
n=len(result)
x=range(n)
plt.plot(x, result,color='b',linewidth=3)
plt.xlabel(&quot;generation&quot;)
plt.ylabel(&quot;loss&quot;)
plt.show()
</code></pre>
<p>下面是运行的结果：</p>
<pre><code>矩阵R为：
 [[5 3 0 1]
 [4 0 0 1]
 [1 1 0 5]
 [1 0 0 4]
 [0 1 5 4]]
矩阵P为：
 [[0.11646557 2.20824426 0.88565195]
 [0.03596535 1.51168656 1.0372382 ]
 [1.6897674  0.11533018 1.13699815]
 [1.37709179 0.16820378 0.84690913]
 [1.50038103 1.43928107 0.61271558]]
矩阵Q为：
 [[-0.09628733 -0.1151309   1.07555694  2.32234973]
 [ 1.97215153  0.90817549  1.96709116 -0.04825554]
 [ 0.89544739  0.69563508  0.70044543  0.95484359]]
预测矩阵：
 [[5.13683287 2.60815509 5.08943398 1.00957287]
 [3.90660419 2.09027523 3.73883681 1.00097713]
 [1.08286742 0.7011314  2.8407112  5.00432095]
 [0.95748942 0.58335244 2.40522644 3.99863775]
 [3.24266723 1.56060602 4.87411612 4.00000375]]

损失函数的收敛过程为：
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://zhangman123456.github.io/post-images/1596076909331.PNG" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[推荐系统之协同过滤算法]]></title>
        <id>https://zhangman123456.github.io/post/tui-jian-xi-tong-zhi-xie-tong-guo-lu-suan-fa/</id>
        <link href="https://zhangman123456.github.io/post/tui-jian-xi-tong-zhi-xie-tong-guo-lu-suan-fa/">
        </link>
        <updated>2020-07-26T03:49:43.000Z</updated>
        <content type="html"><![CDATA[<h1 id="推荐算法">推荐算法</h1>
<h2 id="1什么是推荐算法">1.什么是推荐算法</h2>
<p>首先，在了解协同过滤算法之前，我们可以先了解一下<u>推荐算法</u>的概念。推荐算法是计算机专业中的一种算法，目前应用推荐算法比较好的地方主要是网络，其中淘宝做的比较好，但是作为一个老拼多多用户，我发现拼多多在这方面似乎比淘宝还要强一点。而推荐算法的原理就是利用用户的一些行为，通过一些数学算法，推测出用户可能喜欢的东西，推荐给客户。而近些年由于互联网的爆发，有了更大的数据量可以供我们使用，推荐算法才有了很大的用武之地。</p>
<h2 id="2推荐算法的条件">2.推荐算法的条件</h2>
<p>现在的各种各样的推荐算法，但是不管怎么样，都绕不开几个条件，这是推荐的基本条件<br>
1.根据和你共同喜好的人来给你推荐 。<br>
2.根据你喜欢的物品找出和它相似的来给你推荐 。<br>
3.根据你给出的关键字来给你推荐，而这一部分实际上就是一个搜索算法。<br>
4.根据上面的几种条件组合起来给你推荐。</p>
<h2 id="3推荐算法的分类">3.推荐算法的分类</h2>
<p>推荐算法实际上一共有三种分类：基于内容的推荐算法，协同过滤推荐算法，基于知识的推荐算法。</p>
<h1 id="协同过滤算法">协同过滤算法</h1>
<h2 id="1协同过滤算法理论基础">1.协同过滤算法理论基础</h2>
<p>首先我们要明确一点，在协同过滤中有两种主流算法，一是基于用户的协同过滤，二是基于物品的协同过滤。他们之间的区别在于，基于用户的协同过滤就是将那些和你具有相同喜好的用户喜欢的东西推荐给你，而基于物品的协同过滤是将和你偏好的物品具有相似特征的物品推荐给你。但是实际上，这两者的原理是相同的，就是计算用户以及物品间的相似度，他们的核心部分都是解决相似度问题。所以在阐述这两种分类之前，我们先解决他们共同的核心问题：计算相似度。</p>
<h3 id="1欧式距离的相似度测量">（1）欧式距离的相似度测量</h3>
<p><img src="https://zhangman123456.github.io/post-images/1595787357702.svg" alt="" loading="lazy">　　又称欧几里得距离，用于计算两个样本数据之间的直线距离，这说明，欧式距离更加侧重于两个数据之间的绝对距离，而不太注重这两个数据在方向上的差异</p>
<h3 id="2余弦距离衡量相似度">（2）余弦距离衡量相似度</h3>
<p>余弦相似度原理：用向量空间中的两个向量夹角的余弦值作为衡量两个个体间差异大小的度量，夹角角度越接近0°，余弦距离越接近1，也就是两个向量越相似，相似度越高；夹角角度越接近180°，余弦距离越接近0，也就是越不相似。这就叫做余弦相似。余弦相似度公式的推导比较麻烦，这里附上手写的一份。<img src="https://zhangman123456.github.io/post-images/1595841226337.jpg" alt="" loading="lazy">　　以上我写的是用于计算<strong>两个</strong>样本数据之间的余弦值（相似度），相比欧氏距离，余弦距离更加注重两个向量在方向上的差异。<br>
　　借助三维坐标系来看下欧氏距离和余弦距离的区别：<img src="https://zhangman123456.github.io/post-images/1595842641758.jpg" alt="" loading="lazy">　　从上图可以看出，欧氏距离衡量的是空间各点的绝对距离，跟各个点所在的位置坐标直接相关；而余弦距离衡量的是两个空间向量的夹角，更加体现在方向上的差异。如果保持A点位置不变，B点朝原方向远离坐标轴原点，那么这个时候两点之间的余弦距离是保持不变的，但是欧式距离会发生改变；同样的，也会有欧式距离保持不变，余弦距离发生改变的情况。这就是欧氏距离和余弦距离之间的不同之处。<br>
　　 欧氏距离和余弦距离各自有不同的计算方式和衡量特征，因此它们适用于不同的数据分析模型</p>
<h3 id="3杰卡德相似度">（3）杰卡德相似度</h3>
<h4 id="1杰卡德相似指数">①杰卡德相似指数</h4>
<p>简单来说就是计算两个集合的交集与并集的比值，这个比值结束杰卡德相似指数，用来表示两个集合之间的相似度。公式如下：<img src="https://zhangman123456.github.io/post-images/1595902222896.png" alt="" loading="lazy">不难看出，这个比值介于0~1之间。杰卡德系数越接近1，说明两个集合之间的相似度越高；杰卡德系数越接近0，说明两个集合之间的相似度越低</p>
<h4 id="2杰卡德距离">②杰卡德距离</h4>
<p>杰卡德距离与杰卡德系数刚好相反。<img src="https://zhangman123456.github.io/post-images/1595903967755.png" alt="" loading="lazy">杰卡德距离的取值也是介于0~1之间，但是杰卡德距离用于描述两个集合的不相似度，杰卡德距离越接近1，两个集合越不相似；杰卡德距离越接近0，两个集合越相似。</p>
<h2 id="2基于用户的协同过滤算法">2.基于用户的协同过滤算法</h2>
<h3 id="1算法原理">（1）算法原理</h3>
<p>它一般采用最近邻技术，利用用户的历史喜好信息计算用户之间的距离，然后利用目标用户的最近邻居用户对商品评价的加权评价值来预测目标用户对特定商品的喜好程度，从而根据这一喜好程度来对目标用户进行推荐。通俗来讲，就是将和你喜好相似的用户偏爱的东西推荐给你。</p>
<h3 id="2协同过滤的实现步骤">（2）协同过滤的实现步骤</h3>
<p>思路步骤：<br>
1.计算其他用户和目标用户的相似度（使用欧氏距离算法）；<br>
2.根据相似度的高低找出K个目标用户最相似的邻居；<br>
3.在这些邻居喜欢的物品中，根据邻居与你的远近程度算出每个物品的推荐度；<br>
4.根据每一件物品的推荐度高低给你推荐物品。</p>
<h3 id="3实例">（3）实例</h3>
<p>来看一个例子。A,B,C,D四位用户购买a,b,c,d四种商品的情况如<img src="https://zhangman123456.github.io/post-images/1595965501027.PNG" alt="" loading="lazy">也就是先把每个用户购买商品的情况转换成每个商品的购买情况，然后利用这两份数据，得出一个矩阵，这个矩阵能够大致表明哪两个用户之间是会具有兴趣相似度的，然后计算用户之间的兴趣相似度，根据兴趣相似度较高的用户购买的产品来推荐。<br>
<strong>模块化实现</strong><br>
1.建立倒排矩阵<br>
2.根据倒排矩阵计算每两个用户之间的兴趣相似度（这里采用余弦相似度）<br>
3.排列出和推荐用户兴趣相似度前k个高的用户<br>
4.根据兴趣相似度高的用户购买的产品来推荐并计算推荐度<br>
1.定义</p>
<h3 id="python代码">Python代码</h3>
<pre><code>import math
from operator import *
#例子中的数据相当于是一个用户字典{A:(a,b,d),B:(a,c),C:(b,e),D:(c,d,e)}
#我们这样存储原始输入数据
 
dic={'A':('a','b','d'),'B':('a','c'),'C':('b','e'),'D':('c','d','e')}#简单粗暴，记得加''
#定义一个函数来把用户的共同兴趣储存起来
def Usersim(dicc):#把用户-商品字典转成商品-用户字典（如图中箭头指示那样）
	item_user=dict()#这里创建一个字典，用于存放转换后的数据
	for u,items in dicc.items():#遍历我们输入的这个字典列表中的ABCD,u可以代指字典中的每个键（ABCD）或者说用户,item就是每个ABCD对应的元素
		for i in items:#遍历每一个ABCD中的每个元素。文中的例子是不带评分的，所以用的是元组而不是嵌套字典。（元组类似于列表但是元组的元素不可修改）
			if i not in item_user.keys():#如果这个元素不在新创建的这个字典的键里面（key()表示键）
				item_user[i]=set()#每一个第一次被遍历到的元素（abcd）在此之前都不会存在于这个新的字典中，但是这一行之后就存在了，这样可以将每一个键都设置为set类类型方便操作。
			item_user[i].add(u)#只要已经有了这个元素（abcd中的某一个），就向集合中添加这个元素所在的用户(ABCD)。
	C=dict()#感觉用数组更好一些，真实数据集是数字编号，但这里是字符，这边还用字典。
	N=dict()
	for item,users in item_user.items():
		for u in users:#遍历当前这个键中的每一个元素
			if u not in N.keys():#u是元素，N.key是空的字典的键，所以u是肯定不在N.key里面的，所以这一行是为了初始化
				N[u]=0   #因为这里的u遍历的是N,所以这里的作用还是初始化
			N[u]+=1 #每个商品下用户出现一次就加一次，也就是说，N是用来储存每个用户一共购买的商品个数。
            #但是这个值也可以从最开始的用户表中获得。
            #比如： for u in dic.keys():
            #             N[u]=len(dic[u])
			for v in users:
				if u==v:#这个是用于判断，是否有两个用户购买了相同的产品，相等的话，就＋1
					continue
				if (u,v) not in C.keys():#同上，没有初始值不能进行运算
					C[u,v]=0
				C[u,v]+=1  #这里还是一个字典，不过它的形状已经类似一个矩阵，它储存了所有购买重复的商品的用户的信息，以('A,B','1')的方式储存着。
#到这里倒排阵就建立好了，下面是计算相似度。这里我们采用的是余弦相似度的计算方式
	W=dict()
	for co_user,cuv in C.items():
		W[co_user]=cuv / math.sqrt(N[co_user[0]]*N[co_user[1]])#因为我不是用的嵌套字典，所以这里有细微差别。
	return W#这个W是每两个用户之间的相似度，它会把'A'和'B'之间的距离，'B'和'A'之间的距离都打印出来，尽管这两个距离相等

def Recommend(user,dicc,W2,K):
	rvi=1 #这里都是1,实际中可能每个用户就不一样了。就像每个人都喜欢beautiful girl,但有的喜欢可爱的多一些，有的喜欢御姐多一些。
	rank=dict()#定义一个字典用来储存推荐的商品和该商品的推荐度
	related_user=[]#用于储存每个用户和待推荐用户的兴趣相似度
	interacted_items=dicc[user]#储存待推荐用户已经过购买的商品
	for co_user,item in W2.items():#遍历每一对用户相似度
		if co_user[0]==user:#如果每一组用户相似度里的第一个用户等于我们所求的用户，也就是从所有数据里找出待推荐用户与每个人的兴趣相似度
			related_user.append((co_user[1],item))#先建立一个和待推荐用户兴趣相关的所有的用户列表，也就是储存每个用户和待推荐用户的兴趣相似度
	for v,wuv in sorted(related_user,key=itemgetter(1),reverse=True)[0:K]:#将每个用户和待推荐用户的兴趣相似度从大到小排列，取出前K个
	#在这里，v是在这k个与待推荐用户兴趣相似度最大的人之中遍历，而wuv就是对应的兴趣相似度
		for i in dicc[v]:#遍历&quot;与待推荐用户兴趣相似度最大的用户&quot;所购买的商品
			if i in interacted_items:#这两行是排除待推荐用户和&quot;与待推荐用户兴趣相似度最大的用户&quot;所购买的重复的商品
				continue #如果重复就跳过继续执行
			if i not in rank.keys():#除去重复商品的剩下商品就是要推荐的，在这一行以及之前肯定是还没有储存近rank里，所以这一行的作用是先初始化为零，然后再加上兴趣相似度×具体的系数
				rank[i]=0#先初始化
			rank[i]+=wuv*rvi#然后存入推荐度，也就是兴趣相似度×具体的系数
	return rank
 
if __name__=='__main__':
	W3=Usersim(dic)
	Last_Rank=Recommend('B',dic,W3,2)
    print(W3)
	print (Last_Rank)
</code></pre>
<p>首先是输出每两个用户之间的兴趣相似度：</p>
<pre><code>{('A', 'B'): 0.4082482904638631, ('B', 'A'): 0.4082482904638631, ('A', 'C'): 0.4082482904638631, ('C', 'A'): 0.4082482904638631, ('A', 'D'): 0.3333333333333333, ('D', 'A'): 0.3333333333333333, ('B', 'D'): 0.4082482904638631, ('D', 'B'): 0.4082482904638631, ('C', 'D'): 0.4082482904638631, ('D', 'C'): 0.4082482904638631}
</code></pre>
<p>然后，输入不同的用户，会给出不同的商品和商品推荐度比如输入'A'：</p>
<pre><code>{'c': 0.4082482904638631, 'e': 0.4082482904638631}
</code></pre>
<p>再比如输入b：</p>
<pre><code>{'b': 0.4082482904638631, 'd': 0.8164965809277261, 'e': 0.4082482904638631}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[K-means算法]]></title>
        <id>https://zhangman123456.github.io/post/k-means-suan-fa/</id>
        <link href="https://zhangman123456.github.io/post/k-means-suan-fa/">
        </link>
        <updated>2020-07-20T10:15:14.000Z</updated>
        <content type="html"><![CDATA[<p><strong>1.算法概述</strong>　　<br>
　　K-means，算法，简称聚类算法，通俗来讲，就是把一个区域中的所有点按照距离的远近，划分为若干个类别的算法，直观上表现为聚在一起的点分为一类。<br>
　　我们可以通过以下几张图片来直观感受一下：<br>
对于同样的一组样本数据，如果我们需要将数据分为两类，如下图：<br>
<img src="https://zhangman123456.github.io/post-images/1595435915373.jpg" alt="" loading="lazy"><br>
<strong>2.算法核心思想</strong><br>
　　K-means聚类算法是一种迭代求解的聚类分析算法，其步骤是随机选取K个对象作为初始的聚类中心（就是要把所有的数据分为ｋ个聚类），然后计算每个样本数据与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心，成为这一聚类。聚类中心以及分配给它们的对象就代表一个聚类。每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。这个过程将不断重复直到满足某个终止条件。终止条件可以是没有（或最小数目）对象被重新分配给不同的聚类，没有（或最小数目）聚类中心再发生变化，误差平方和局部最小。<br>
<strong>3. 算法实现步骤</strong><br>
　　1、首先确定一个k值，即我们希望将数据集经过聚类得到k个集合。<br>
　　2、从数据集中<u>随机选择</u>k个数据点作为质心。<br>
　　3、对数据集中每一个点，计算其与每一个质心的距离（如欧式距离），离哪个质心近，就划分到那个质心所属的集合。<br>
　　4、把所有数据归好集合后，一共有k个集合。然后重新计算每个集合的质心。<br>
　　5、如果新计算出来的质心和原来的质心之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于稳定，或者说收敛），我们可以认为聚类已经达到期望的结果，算法终止。<u>（在这里我们规定终止条件为聚类中心不再发生变化。在不同的实验中我们可以规定不同的终止条件）</u><br>
　　6、如果新质心和原质心距离变化很大，需要迭代3~5步骤。<br>
<strong>4. K-means算法优缺点</strong><br>
　　优点：<br>
　　1、原理比较简单，实现也是很容易，收敛速度快。<br>
　　2、当结果簇是密集的，而簇与簇之间区别明显时, 它的效果较好。<br>
　　3、主要需要调参的参数仅仅是簇数k。<br>
　　缺点：<br>
　　1、K值需要预先给定，很多情况下K值的估计是非常困难的。<br>
　　2、K-Means算法对初始选取的质心点是敏感的，不同的随机种子点得到的聚类结果完全不同 ，对结果　　影响很大。<br>
　　3、对噪音和异常点比较的敏感。用来检测异常值。<br>
　　4、采用迭代方法，可能只能得到局部的最优解，而无法得到全局的最优解。<br>
5.代码实现（自己的注释和理解）</p>
<pre><code>#导入numpy库
from numpy import *
#K-均值聚类辅助函数
def numpy import *#文本数据解析函数
    dataMat=[]
    fr=open(fileName)
    for line in fr.readlines():#读取文件的每一行直到结束，然后返回一个列表
        curLine=line.strip().split('\t')#以制表的分隔符为界限分开，去掉前后的空格或者换行符，制表符
        fltLine=map(float,curLine)#将每一行的数据映射成float型
        dataMat.append(fltLine)#将数据添加至
    return dataMat
   
def distEclud(vecA,vecB):
    return sqrt(sum(power(vecA-vecB,2)))#数据向量计算欧式距离 

#随机初始化K个聚类中心(满足数据边界之内)
def randCent(dataSet,k):#定义样本数据和k值为变量
    n=shape(dataSet)[1]#得到数据样本的维度
    centroids=mat(zeros((k,n)))#初始化为一个(k,n)的矩阵，每一行的元素个数都为样本数据的维度。因为我们取得的k值和样本数据的维度相同，所以在取k值时，是在每一个维度都随机取值
    for j in range(n):#遍历数据集的每一维度，计算所有k值每一维度的取值。也就是第一次循环计算所有k值的第一维度；第二次循环计算所有k值的第二维度；...一直到第n次循环计算所有k值的第n维度。
        minJ=min(dataSet[:,j]) #得到该列数据的最小值
        rangeJ=float(max(dataSet[:,j])-minJ)#得到该列数据的取值范围(最大值-最小值)
        #k个质心向量的第j维数据值随机为位于(最小值，最大值)内的某一值
        centroids[:,j]=minJ+rangeJ*random.rand(k,1)#也就是先计算所有k值在这一维度的取值。取值 = 最小值+（最大值-最小值）*（0~1）随机数
    return centroids#返回初始化得到的k个聚类中心
    
#k-均值聚类算法
#@dataSet:聚类数据集
#@k:用户指定的k个类
#@distMeas:距离计算方法，默认欧氏距离distEclud()
#@createCent:获得k个质心的方法，默认随机获取randCent()（将randCent()赋值给createCent
def kMeans(dataSet,k,distMeas=distEclud,createCent=randCent):
    m=shape(dataSet)[0]#获取数据集样本数
    clusterAssment=mat(zeros((m,2)))#初始化一个(m,2)的矩阵，第一列用于放置每一个样本数据归于哪一个聚类中心（也就是当前样本的聚类结果），第二列用于放置平方误差
    centroids=createCent(dataSet,k)#centroids = createCent() = randCent()，即引用之前创建的函数，创建初始的k个质心向量
    clusterChanged=True#聚类结果是否发生变化的布尔类型
    while clusterChanged:#只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不变化
        clusterChanged=False#聚类结果变化布尔类型置为false（也就是是说聚类结果并未发生变化）
        for i in range(m): #遍历数据集每一个样本向量
            minDist=inf;minIndex=-1#每当对一个样本数据进行距离计算时，就初始化最小距离为正无穷；最小距离对应索引为-1（也就是初始化索引为-1，为了方便以后依次叠加搜寻每一个聚类中心
            for j in range(k):#循环k个类的聚类中心
                distJI = distMeas(centroids[j,:],dataSet[i,:])#计算一个数据点到一个聚类中心的欧氏距离
                if distJI&lt;minDist:#如果距离小于当前最小距离（所以初始化最小距离为正无穷是为了方便留下第一个被计算的距离）
                    minDist=distJI;minIndex=j#直接更新当前距离为当前最小距离；最小距离对应索引对应为j(第j个聚类中心)。重复这样的迭代可以得出最小的距离
        if clusterAssment[i,0] !=minIndex:clusterChanged=True#当前聚类结果中第i个样本的聚类结果发生变化：布尔类型置为true，继续聚类算法
        clusterAssment[i,:]=minIndex,minDist**2#更新当前变化样本的聚类结果和平方误差
    print centroids#打印k-均值聚类的质心
    for cent in range(k):#遍历每一个质心
        ptsInClust=dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]#将数据集中所有属于当前聚类中心的样本通过条件过滤筛选出来
        centroids[cent,:]=mean(ptsInClust,axis=0)#计算这些数据的均值（axis=0：求列的均值），作为该类的聚类中心，也就是更新聚类中心
    return centroids,clusterAssment  #返回k个聚类，聚类结果及误差
    ```
6.由1.可以看出，k的取值对于我们分类的结果起着非常重要的作用。在应用中，k值一般来说是根据实际情况的需要，人为的确定k值。同时，我们也可以通过不停的调试K值的大小，观察不同的k值产生的分类结果以及误差来判断比较合适的k值</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[KNN算法]]></title>
        <id>https://zhangman123456.github.io/post/knn-suan-fa/</id>
        <link href="https://zhangman123456.github.io/post/knn-suan-fa/">
        </link>
        <updated>2020-07-16T08:45:34.000Z</updated>
        <content type="html"><![CDATA[<p><strong>１.算法简述</strong><br>
　　邻近算法，或者说K最近邻（KNN，K-NearestNeighbor）<u>分类算法</u>是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻近值来代表。近邻算法就是将数据集合中每一个记录进行分类的方法。<br>
<strong>２.算法详述</strong><br>
我们可以通过几幅图来形象地了解KNN算法<img src="https://zhangman123456.github.io/post-images/1594891375739.png" alt="" loading="lazy">　　毫无疑问，k值的选定对于分类的效果至关重要。如果K值过小，拟合的效果不好；如果K值过大，就会覆盖过多的样本数据，失去拟合的意义。<br>
　　要度量空间中点距离的话，有好几种度量方式，比如常见的曼哈顿距离计算，欧式距离（或欧几里得距离）等等。不过通常KNN算法中使用的是欧式距离，这里只是简单说一下，拿二维平面为例，，二维空间两个点的欧式距离计算公式如下：<img src="https://zhangman123456.github.io/post-images/1594891829868.jpg" alt="" loading="lazy">　　这样一来，不难得出，KNN算法最简单粗暴的方式就是计算预测点和所有样本点之间的距离，选出前K个距离最小的点看看哪个种类的点的数量最多。<br>
<strong>3.具体步骤</strong><br>
1.计算所有测试数据到一个训练数据之间的距离<br>
2.将这些距离按照从小到大的顺序储存起来<br>
3.取出最小的前k个点，<br>
4.确定这k个点出现的类别的频率<br>
5.返回前K个点中出现频率最高的类别作为测试数据的预测分类。<br>
6.重复步骤1-5<br>
<strong>4.KNN特点</strong><br>
KNN是一种<strong>非参</strong>的，<strong>惰性</strong>的算法模型。什么是非参，什么是惰性呢？<br>
<strong>非参</strong>的意思并不是说这个算法不需要参数，而是意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。</p>
<p><strong>惰性</strong>又是什么意思呢？想想看，同样是分类算法，逻辑回归需要先对数据进行大量训练（tranning），最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。</p>
<p><strong>KNN算法的优势和劣势</strong><br>
了解KNN算法的优势和劣势，可以帮助我们在选择学习算法的时候做出更加明智的决定。那我们就来看看KNN算法都有哪些优势以及其缺陷所在！<br>
<strong>KNN算法优点</strong><br>
简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。<br>
模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。<br>
预测效果好。<br>
对异常值不敏感<br>
<strong>KNN算法缺点</strong><br>
对内存要求较高，因为该算法存储了所有训练数据<br>
预测阶段可能很慢<br>
对不相关的功能和数据规模敏感</p>
<p>KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。因此，基于python已经有很多成熟的可以直接使用的库来进行算法的实现<br>
KNN算法的简单实现：（调用sklearn自带的库）</p>
<pre><code>from sklearn import neighbors #sklearn中的函数集
from sklearn import datasets  #sklearn中的数据集

knn = neighbors.KNeighborsClassifier() # 申明对象
iris = datasets.load_iris()  # 导入数据（一个数据集，很大的数据集）
print(iris)

knn.fit(iris.data,iris.target) # 生成KNN模型，fit()函数可以理解为一个训练的过程

predicit_label = knn.predict([[0.2,0.3,0.3,0.2]]) # 预测这个数组的概率
print(predicit_label)
</code></pre>
<p>KNN算法的具体实现：<br>
（也是copy别人的代码，自己加的注释）</p>
<pre><code>'''
导入数据
filename数据存储路径
radio，按指定比例将数据划分为训练集和测试集
trainSet：训练数据
testSet：测试数据
'''
def loadDateset(filename,radio,trainSet=[],testSet=[]):
    with open(filename,'rt') as csvfile:   #打开filename；csvfile是一种文件格式
        lines = csv.reader(csvfile)   # 逐行读取数据
        dataset = list(lines)        # 将文件中的样本数据转换为列表类型存储
        for x in range(len(dataset)-1):  # 循环每行数据，将前4个特征值存入数组
            for y in range(4):
                dataset[x][y] = float(dataset[x][y])#也就是将前四列更新成浮点数
            if random.random()&lt;radio:    # 根据radio的大小将样本数据集划分为训练数据和测试数据
                trainSet.append(dataset[x])
            else:
                testSet.append(dataset[x])
'''
计算2个样例之间的距离（欧氏距离），length表示数据的维度
'''
def evaluateDistance(instance1,instance2,length):#变量为两个样例，还有维度
    distance = 0#初始化距离
    for x in range(length): # 循环每一维度，数值相减并对其平方，然后进行累加
        distance += pow((instance1[x]-instance2[x]),2)
    return math.sqrt(distance) # 开方求距离

'''
对于一个实例，找到离他最近的k个实例
'''
def getNeighbors(trainSet,testInstance,k):
    distance = []
    length = len(testInstance)-1 # 每个测试实例的维度（比如（2,3,4）就是三个维度）
    for x in range(len(trainSet)-1): 
        dist = evaluateDistance(testInstance,trainSet[x],length)# 分别计算一个测试实例到每一个训练数据的距离
        distance.append((trainSet[x],dist)) # 将每一个训练实例和其对应到测试实例的距离存储到列表（append（）函数用于在列表末尾增添新的对象）
    distance.sort(key=operator.itemgetter(1)) #进行排序，而排序的关键是元素第二维数据的大小，也就是dist
    neighbors = [] # 用来存储离一个实例最近的几个测试数据
    for x in range(k): # 取distance中前k个实例存储到neighbors
        neighbors.append(distance[x][0])#已经经过排序的distance列表，取出前k个元素，就是一个测试实例的k个近邻
        return neighbors

'''
在最近的K个实例中投票，少数服从多数，把要预测的实例归到多数那一类
'''
def getResponse(neighbors):#neighbors作为变量
    classvotes = {} # 定义一个字典，存储每一类别的数目
    for x in range(len(neighbors)):#遍历每一个k近邻的距离值大小
        response = neighbors[x][-1]#表示第x行最后一列的元素
        if response in classvotes:
            classvotes[response] += 1
        else:
            classvotes[response] = 1
    sortedVotes = sorted(classvotes.items(),key=operator.itemgetter(1),reverse=True) # 排序，输出数目最大的类别
    return sortedVotes[0][0]

'''
计算测试集的准确率
'''
def getAccuracy(testSet,predictions):
    correct = 0
    for x in range(len(testSet)):
        if testSet[x][-1] == predictions[x]: # 每行测试用例最后一列的标签与预测标签是否相等
            correct += 1
    return (correct/float(len(testSet)))*100.0

def main():
    trainSet = [] # 存储训练集
    testSet = []  # 存储测试集
    radio = 0.80  # 按4：1划分
    loadDateset('G:/PycharmProjects/Machine_Learning/KNN/irisdata.txt',radio,trainSet,testSet) #导入数据并划分
    print(&quot;trainSetNum: &quot;+ str(len(trainSet)))
    print(&quot;testSetNum: &quot;+  str(len(testSet)))
    predictions = []
    k = 3 # 选取前k个最近的实例
    for x in range(len(testSet)): # 循环预测测试集合的每个实例
        neighbors = getNeighbors(trainSet,testSet[x],k)
        result = getResponse(neighbors)
        predictions.append(result)
        print('&gt;predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))
    accuracy = getAccuracy(testSet,predictions)
    print('Accuracy: ' + repr(accuracy) + '%')

if __name__ == '__main__':
    main()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[线性回归]]></title>
        <id>https://zhangman123456.github.io/post/xian-xing-hui-gui/</id>
        <link href="https://zhangman123456.github.io/post/xian-xing-hui-gui/">
        </link>
        <updated>2020-07-11T10:59:13.000Z</updated>
        <content type="html"><![CDATA[<p>1.该<u>样本回归方程</u>就是我我们需要拟合的曲线：<br>
                     <strong>h(x) = θ<sub>0</sub> + θ<sub>1</sub>x</strong>；<br>
2.得出关于该回归方程的误差函数:<img src="https://zhangman123456.github.io/post-images/1594575102477.gif" alt="" loading="lazy"><br>
3.对误差函数求导：<br>
<img src="https://zhangman123456.github.io/post-images/1594575689326.gif" alt="" loading="lazy"><br>
4.之后更新的到的θ<sub>0</sub> 和 θ<sub>1</sub><img src="https://zhangman123456.github.io/post-images/1594575337724.gif" alt="" loading="lazy">（公式里的α是学习率，我们可以象征性地理解为步长）<br>
5.接下来，利用梯度下降的方法。这里我们可以分别使用批量梯度下降和随机梯度下降的方法来实现**（有助于更加具体地理解这两种梯度下降的算法）**<br>
（1）批量梯度下降算法<img src="https://zhangman123456.github.io/post-images/1594577818861.png" alt="" loading="lazy"><u>（上标i代表具体的某一个数据；x<sub>j</sub>代表系数θ<sub>j</sub>对应的因变量）</u><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，都需要把全部的样本都使用一遍，这样重复迭代直到θ<sub>j</sub>收敛为止。所以说该算法的复杂度是O(i（j+1）)<br>
（为了防止我忘了，我写了一个一看就懂的版本）<br>
  <img src="https://zhangman123456.github.io/post-images/1594579705255.jpg" alt="" loading="lazy"><br>
（2）随机梯度下降法<img src="https://zhangman123456.github.io/post-images/1594579776426.png" alt="" loading="lazy"><br>
  每一次的迭代，都把从θ<sub>0</sub> 到 θ<sub>j</sub>的每一个参数进行迭代；而每一次对每一个参数进行迭代时，只使用所有样本中的一个数据，一旦到达最大的迭代次数或是满足预期的精度，就停止。这样算法的复杂度为O(j+1)，就下降了很多。<br>
  <u>（还是一个一看就会的版本）</u><img src="https://zhangman123456.github.io/post-images/1594580748171.jpg" alt="" loading="lazy"></p>
<pre><code>批量梯度下降算法
@author: 
&quot;&quot;&quot;

import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
 
#数据
a = np.random.standard_normal((1, 500))
x = [150,200,250,300,350,400,600]#自变量x
y = [6450,7450,8450,9450,11450,15450,18450]#因变量y
y = y - a*10
y = y[0]
 
def Optimization(x,y,theta,learning_rate):
    for i in range(iter):#批量梯度下降算法的核心，iter是最大迭代的次数。
        theta = Updata(x,y,theta,learning_rate)更新参数用的函数
    return theta#这是全部迭代之后的最后一组参数值 
def Updata(x,y,theta,learning_rate):
    m = len(x)#因变量的长度，即每次迭代的次数
    sum = 0.0
    sum1 = 0.0
    alpha = learning_rate
    h = 0
    for i in range(m):#每一次迭代，都用到全部的样本数据
        h = theta[0] + theta[1] * x[i]#一元线性回归方程，即我们需要拟合的曲线。
        sum += (h - y[i])
        sum1 += (h - y[i]) * x[i]
    theta[0] -= alpha * sum / m #更新theta[0]
    theta[1] -= alpha * sum1 / m #更新theta[1]
    return theta#每一次迭代都输出更新后的参数
#数据初始化
learning_rate = 0.001#学习率
theta = [0,0]#最开始的参数值
iter = 1000#循环的次数
theta = Optimization(x,y,theta,learning_rate)
 
plt.rcParams['font.sans-serif']=['SimHei']#该函数用来定义图形的各种默认属性，比如字符显示，线条样式，窗口大小，坐标轴宽度等等属性
plt.rcParams['axes.unicode_minus'] = False

plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('样本',fontsize=40)
plt.savefig(&quot;样本.jpg&quot;)
'''
#可视化
b = np.arange(0,50)
c = theta[0] + b * theta[1]
 
plt.figure(figsize=(35,35))
plt.scatter(x,y,marker='o')
plt.plot(b,c)
plt.xticks(fontsize=40)
plt.yticks(fontsize=40)
plt.xlabel('特征X',fontsize=40)
plt.ylabel('Y',fontsize=40)
plt.title('结果',fontsize=40)
plt.savefig(&quot;结果.jpg&quot;)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[梯度下降法]]></title>
        <id>https://zhangman123456.github.io/post/ti-du-xia-jiang-fa-yi/</id>
        <link href="https://zhangman123456.github.io/post/ti-du-xia-jiang-fa-yi/">
        </link>
        <updated>2020-07-11T01:15:53.000Z</updated>
        <content type="html"><![CDATA[<p>#基本思想<br>
假设我们爬山，如果想最快的上到山顶，那么我们应该从山势最陡的地方上山。也就是山势变化最快的地方上山<br>
同样，如果从任意一点出发，需要最快搜索到函数最大值，那么我们也应该从函数变化最快的方向搜索。<br>
函数变化最快的方向是什么呢？函数的<strong>梯度</strong>。<br>
假如函数为一元函数，<strong>梯度</strong>就是该函数的倒数<img src="https://zhangman123456.github.io/post-images/1594430603494.png" alt="" loading="lazy"><br>
如果为二元函数，梯度定义为：<img src="https://zhangman123456.github.io/post-images/1594430651348.png" alt="" loading="lazy">（此处用到高数偏导数的知识来求二元函数的倒数）<br>
如果需要找的是函数极小点，那么应该从负梯度的方向寻找，该方法称之为梯度下降法。<br>
<img src="https://zhangman123456.github.io/post-images/1594430871728.png" alt="" loading="lazy">要搜索极小值C点，在A点必须向x增加方向搜索，此时与A点梯度方向相反；在B点必须向x减小方向搜索，此时与B点梯度方向相反。总之，搜索极小值，必须向负梯度方向搜索。<br>
假设函数 y = f(x1,x2,......,xn) 只有一个极小点。初始给定参数为  X0 = (x10,x20,......xn0) 从这个点如何搜索才能找到原函数的极小值点？<br>
方法：</p>
<ol>
<li>首先设定一个较小的正数η，ε;</li>
<li>求当前位置处的各个偏导数：<img src="https://zhangman123456.github.io/post-images/1594431281163.png" alt="" loading="lazy"></li>
<li>修改当前函数的参数值，公式如下：<br>
<img src="https://zhangman123456.github.io/post-images/1594431327433.png" alt="" loading="lazy"></li>
<li>如果参数变化量小于，退出；否则返回2。<br>
例：任给一个初始出发点，设为x0=-4，利用梯度下降法求函数y=x²/2-2x的极小值。<br>
准备工作：<br>
（1）给定两个参数值η = 0.9，ε = 0.01（η即为学习效率，或者步长）<br>
（2）计算导数：dy/dx = x - 2;<br>
一.（1）计算当前导数值：y' = -4 - 2 = -6；<br>
   （2）修改当前的参数值：x = x-ηy' = -4 - 0.9*(-6) = 1.4;<br>
  （3）计算△x = -0.9 * （-6）= 5.4。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环<br>
二.（1）计算当前导数值：y' = 1.4 - 2 = -0.6；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.4 - 0.9*(-0.6)= 1.94;<br>
  （3）计算△x = -0.9 * （-0.6）= 0.54。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环。<br>
三.（1）计算当前导数值：y' = 1.94 - 2 = -0.06；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.94 - 0.9*(-0.06)= 1.994;<br>
  （3）计算△x = -0.9 * （-0.06）= 0.054。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&gt;ε，继续进行循环。<br>
四.（1）计算当前导数值：y' = 1.994 - 2 = -0.006；<br>
   （2）修改当前的参数值：x = x-ηy' = 1.994 - 0.9*(-0.006)= 1.9994;<br>
  （3）计算△x = -0.9 * （-0.006）= 0.0054。<br>
  （4）将△x与ε进行比较，如果△x&lt;=ε ，则变化量满足终止条件，终止循环，输出参数x；<br>
              如果△x&gt;ε，则继续进行循环<br>
              通过比较，可知△x&lt;=ε，则终止循环，输出此时的x即为函数的极小点。<br>
python代码实现</li>
</ol>
<pre><code class="language-python">＃解决实例的Python代码
import numpy as np
inport matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams['font.family'] = 'sans-serif'
mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = false

#构建一维元素图形
def f1(x):
　　return 0.5 * (x ** 2) - 2x
def h1(x):
　　return x-2

#使用梯度下降法
GD_X = []
GD_Y = []
x = -4
alpha = 0.9
f_change = np.abs(alpha * hl(x))
f_current = f_change
GD_X.append(x)
GD_Y.append(f_current)

#迭代次数
iter_num = 0
while f_change &gt;0.01 and iter_num&lt;10:
　　iter_num+=1
　　x = x - x - alpha * h1(x)
　　tem = f1(x)
　　f_change = np.abs(alpha * hl(x))
　　f_current = tmp
　　GD_X.append(f_current)

print(u'最终的结果：(%.5f,%.5f)'%(x,f_current))
pirnt(u'迭代次数：%d'%iter_num)
printf(GD_X)
</code></pre>
<p>#损失函数<br>
学习了梯度下降之后，我们引入新的概念：<strong>损失函数</strong>。便于我们直接利用梯度下降法计算损失函数的最小值。<br>
我们可以近似地将<strong>损失函数</strong>理解为方差或者平方差。根据给出的一系类数据我们可以得出相关的函数，再根据该函数和不同变量，我们可以得出不同变量对应的不同的函数值。而每个函数值和每个真实的数值之间有一定的误差，根据这些误差我们可以得到相关的函数，即<strong>损失函数</strong>。<strong>损失函数</strong>的值越小，我们之前得到的函数就越精确。<br>
例：根据某组数据得到预测的函数：h(x) = θ₁ + θ₂x。只含有一个特征/输入变量。在这个式子中θ₁，θ₂都是未知参数，x是已知数据。我们的目的是得到合适的参数使得我们的预测尽可能准确。这时就需要引入损失函数的概念。<br>
<img src="https://zhangman123456.github.io/post-images/1594383086368.jpg" alt="" loading="lazy"><em>(暂时不用管1/2)</em><br>
上图中的J(θ₁,θ₂)是一个关于θ₁和θ₂的二元函数，也就是<strong>损失函数</strong>。在三维空间坐标系内，存在一对点（θ₁,θ₂）使得J(θ₁,θ₂)有最小值<br>
然后对损失函数进行一个简单的推导（利用梯度下降的算法）<br>
<img src="https://zhangman123456.github.io/post-images/1594444209052.jpg" alt="" loading="lazy">（在这个公式里突然多了一个α，它表示的是学习率，用来限定步长的大小，也很容易理解，毕竟得到的偏导是类似斜率的东西，只是一个方向，总得加个数值，才能表示向这个方向移动的距离。<br>
对于的取值，一般都取的比较小，但也不要太小，太小就意味着迭代步数要增加，运算时间边长。另外，迭代的步数要自己设置。<br>
原文链接：https://blog.csdn.net/i96jie/java/article/details/81206473）<br>
#梯度下降法的三个变种<br>
了解了梯度下降法的核心思想以及损失函数后，我们接着了解梯度下降法的三个变种<br>
1.批量梯度下降法（BGD）<br>
其需要计算整个训练集的梯度，即：<img src="https://i.loli.net/2019/08/12/zAiXFLUSyrd5qap.png" alt="" loading="lazy">其中η为学习率，用来控制更新的“力度”/&quot;步长&quot;。<br>
优点：<br>
    对于凸目标函数，可以保证全局最优； 对于非凸目标函数，可以保证一个局部最优。<br>
2.随机梯度下降算法（SGD）<br>
仅计算某个样本的梯度，即针对某一个训练样本 xi及其label yi更新参数：<img src="https://i.loli.net/2019/08/12/ItgcSRaT4jreKMl.png" alt="" loading="lazy">逐步减小学习率，SGD表现得同BGD很相似，最后都可以有不错的收敛。<br>
优点：<br>
    更新频次快，优化速度更快; 可以在线优化(可以无法处理动态产生的新样本)；一定的随机性导致有几率跳出局部最优(随机性来自于用一个样本的梯度去代替整体样本的梯度)。<br>
3.小批量梯度下降算法(MBGD),计算包含n个样本的mini-batch的梯度：<img src="https://i.loli.net/2019/08/12/ItgcSRaT4jreKMl.png" alt="" loading="lazy">MBGD是训练神经网络最常用的优化方法。<br>
优点：<br>
参数更新时的动荡变小，收敛过程更稳定，降低收敛难度；可以利用现有的线性代数库高效的计算多个样本的梯度。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://zhangman123456.github.io/post/hello-gridea/</id>
        <link href="https://zhangman123456.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>